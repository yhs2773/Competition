{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393159b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/0_felton'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data_ex_total')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # 여기서 'cuda'가 출력되어야 GPU와 연결이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train path image 확인 각폴더당 1개\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # 파일이름을 출력합니다\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # 이미지 정보를 출력합니다\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        break # 폴더마다 1장만 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed9c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 전처리 함수 만들기 resize 224, 224 -> resNet 사용 shape조절\n",
    "\n",
    "# train data image 증강\n",
    "# random H-flip, V-flip image 증강\n",
    "# 색깔 바꾸기 colorjitter \n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e847514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'clean': 0, 'dirt': 1}\n"
     ]
    }
   ],
   "source": [
    "# 위에서 만든 함수로 데이터셋 준비\n",
    "# train dataset\n",
    "# batch_size = 32\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992bfc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_total/train : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_total/train/dirt : 3926\n",
      "/aiffel/aiffel/0_felton/data_ex_total/train/clean : 3761\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스별 이미지 개수 확인\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{dirpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431e0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  {'clean': 0, 'dirt': 1}\n"
     ]
    }
   ],
   "source": [
    "# test data set 준비\n",
    "# shuffle = False\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ccc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_total/test : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_total/test/dirt : 600\n",
      "/aiffel/aiffel/0_felton/data_ex_total/test/clean : 600\n"
     ]
    }
   ],
   "source": [
    "# 개수 확인\n",
    "for dirpath, dirnames, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{dirpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0425de",
   "metadata": {},
   "source": [
    "### model 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a98d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 함수\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709bb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = out.max(1)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label).sum().item() \n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 함수\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5a50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 함수\n",
    "# train -> test -> save 반복\n",
    "# 가장 좋은 accuracy 를 저장\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c3c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /aiffel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec16f7fd564247bd9deda40654a3a0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 가져오기\n",
    "# 사용모델 resNet 50, pretrained = True\n",
    "# pretraine 된 모델로 전이 학습예정\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb91bd",
   "metadata": {},
   "source": [
    "### 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd89e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.27876008037744376\n",
      "Acc:  0.8850006504488097\n",
      "F1:  0.8849250717513424\n",
      "Precision:  0.8850226249310439\n",
      "Recall:  0.8848582449877371\n",
      "[[3303  458]\n",
      " [ 426 3500]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.6127794181946696\n",
      "Test Acc:  0.7408333333333333\n",
      "Test F1:  0.7320718983481357\n",
      "Test Precision:  0.7770755901837902\n",
      "Test Recall:  0.7408333333333333\n",
      "[[336 264]\n",
      " [ 47 553]]\n",
      "[Notification] Best Model Updated!\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch  시험 \n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6f9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.10919307575163072\n",
      "Acc:  0.9652660335631585\n",
      "F1:  0.96523326373478\n",
      "Precision:  0.9655800115669408\n",
      "Recall:  0.965051674537844\n",
      "[[3592  169]\n",
      " [  98 3828]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.015413598019474162\n",
      "Test Acc:  0.995\n",
      "Test F1:  0.9949999861110724\n",
      "Test Precision:  0.9950055000611118\n",
      "Test Recall:  0.995\n",
      "[[598   2]\n",
      " [  4 596]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.06023066104640914\n",
      "Acc:  0.9802263561857681\n",
      "F1:  0.9802136234498902\n",
      "Precision:  0.9803237312392902\n",
      "Recall:  0.9801334323376509\n",
      "[[3670   91]\n",
      " [  61 3865]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2928140545806552\n",
      "Test Acc:  0.8691666666666666\n",
      "Test F1:  0.866888142941221\n",
      "Test Precision:  0.8963011889035667\n",
      "Test Recall:  0.8691666666666666\n",
      "[[443 157]\n",
      " [  0 600]]\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.04062240491273399\n",
      "Acc:  0.9876414726161051\n",
      "F1:  0.9876352820930645\n",
      "Precision:  0.9876559841381499\n",
      "Recall:  0.987616220472249\n",
      "[[3710   51]\n",
      " [  44 3882]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.17287851192499024\n",
      "Test Acc:  0.9416666666666667\n",
      "Test F1:  0.9415105322820083\n",
      "Test Precision:  0.9464335852828536\n",
      "Test Recall:  0.9416666666666667\n",
      "[[534  66]\n",
      " [  4 596]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03737683383809766\n",
      "Acc:  0.989202549759334\n",
      "F1:  0.9891963597401001\n",
      "Precision:  0.9892603333245271\n",
      "Recall:  0.9891444935237008\n",
      "[[3710   51]\n",
      " [  32 3894]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01289301508465843\n",
      "Test Acc:  0.9941666666666666\n",
      "Test F1:  0.9941664681645417\n",
      "Test Precision:  0.9942339373970346\n",
      "Test Recall:  0.9941666666666666\n",
      "[[593   7]\n",
      " [  0 600]]\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03308615352933322\n",
      "Acc:  0.9905034473786913\n",
      "F1:  0.990498579124006\n",
      "Precision:  0.9905259507508744\n",
      "Recall:  0.9904739271849612\n",
      "[[3720   41]\n",
      " [  32 3894]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.060408569175790634\n",
      "Test Acc:  0.9825\n",
      "Test F1:  0.9824985293903168\n",
      "Test Precision:  0.9826622281377908\n",
      "Test Recall:  0.9825\n",
      "[[584  16]\n",
      " [  5 595]]\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03752767687462487\n",
      "Acc:  0.9899830883309484\n",
      "F1:  0.9899784148676756\n",
      "Precision:  0.9899811858979569\n",
      "Recall:  0.9899756773914872\n",
      "[[3722   39]\n",
      " [  38 3888]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.024388278103005864\n",
      "Test Acc:  0.9966666666666667\n",
      "Test F1:  0.9966666666666667\n",
      "Test Precision:  0.9966666666666667\n",
      "Test Recall:  0.9966666666666667\n",
      "[[598   2]\n",
      " [  2 598]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03281998947403815\n",
      "Acc:  0.9898529985690127\n",
      "F1:  0.9898483213038662\n",
      "Precision:  0.9898483213038662\n",
      "Recall:  0.9898483213038662\n",
      "[[3722   39]\n",
      " [  39 3887]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.09083216931243063\n",
      "Test Acc:  0.9733333333333334\n",
      "Test F1:  0.9733143568760007\n",
      "Test Precision:  0.9746835443037974\n",
      "Test Recall:  0.9733333333333334\n",
      "[[568  32]\n",
      " [  0 600]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.020619462138665955\n",
      "Acc:  0.9929751528554702\n",
      "F1:  0.992971755769859\n",
      "Precision:  0.9929833098656893\n",
      "Recall:  0.9929607401918203\n",
      "[[3732   29]\n",
      " [  25 3901]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.06375944635070785\n",
      "Test Acc:  0.9958333333333333\n",
      "Test F1:  0.9958332609941145\n",
      "Test Precision:  0.9958677685950413\n",
      "Test Recall:  0.9958333333333333\n",
      "[[595   5]\n",
      " [  0 600]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.021803807935298757\n",
      "Acc:  0.9937556914270846\n",
      "F1:  0.9937527428766477\n",
      "Precision:  0.9937583946525623\n",
      "Recall:  0.9937472258315665\n",
      "[[3736   25]\n",
      " [  23 3903]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01961125541255796\n",
      "Test Acc:  0.995\n",
      "Test F1:  0.9949998749968749\n",
      "Test Precision:  0.995049504950495\n",
      "Test Recall:  0.995\n",
      "[[594   6]\n",
      " [  0 600]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.01833334465207304\n",
      "Acc:  0.9947964095225705\n",
      "F1:  0.9947939523972065\n",
      "Precision:  0.9947996160199906\n",
      "Recall:  0.9947884236465545\n",
      "[[3740   21]\n",
      " [  19 3907]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.17858559139321908\n",
      "Test Acc:  0.9383333333333334\n",
      "Test F1:  0.9381346769069568\n",
      "Test Precision:  0.9440367385529693\n",
      "Test Recall:  0.9383333333333334\n",
      "[[529  71]\n",
      " [  3 597]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.020438956053293777\n",
      "Acc:  0.9941459607128919\n",
      "F1:  0.9941431632268907\n",
      "Precision:  0.9941517483495971\n",
      "Recall:  0.9941348813729345\n",
      "[[3737   24]\n",
      " [  21 3905]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.8235675794673928\n",
      "Test Acc:  0.5658333333333333\n",
      "Test F1:  0.466876323322626\n",
      "Test Precision:  0.7556330963963285\n",
      "Test Recall:  0.5658333333333334\n",
      "[[ 81 519]\n",
      " [  2 598]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.021424765256136902\n",
      "Acc:  0.9934955119032132\n",
      "F1:  0.9934921390254521\n",
      "Precision:  0.9935230686451093\n",
      "Recall:  0.9934645772637993\n",
      "[[3731   30]\n",
      " [  20 3906]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.003968087021833325\n",
      "Test Acc:  0.9983333333333333\n",
      "Test F1:  0.9983333287036908\n",
      "Test Precision:  0.9983388704318936\n",
      "Test Recall:  0.9983333333333333\n",
      "[[598   2]\n",
      " [  0 600]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.020393313619035732\n",
      "Acc:  0.9949264992845063\n",
      "F1:  0.9949241322057059\n",
      "Precision:  0.9949269311206108\n",
      "Recall:  0.9949213670126806\n",
      "[[3741   20]\n",
      " [  19 3907]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01743213789693204\n",
      "Test Acc:  0.9975\n",
      "Test F1:  0.9974999843749024\n",
      "Test Precision:  0.9975124378109452\n",
      "Test Recall:  0.9975\n",
      "[[600   0]\n",
      " [  3 597]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.013183896661459766\n",
      "Acc:  0.9964875764277351\n",
      "F1:  0.9964858979361345\n",
      "Precision:  0.9964945233997116\n",
      "Recall:  0.9964775764566577\n",
      "[[3746   15]\n",
      " [  12 3914]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.009403563717106303\n",
      "Test Acc:  0.9966666666666667\n",
      "Test F1:  0.9966666296292181\n",
      "Test Precision:  0.9966887417218543\n",
      "Test Recall:  0.9966666666666666\n",
      "[[596   4]\n",
      " [  0 600]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.01722207148868544\n",
      "Acc:  0.9946663197606348\n",
      "F1:  0.9946638312931779\n",
      "Precision:  0.9946666287404711\n",
      "Recall:  0.9946610675589336\n",
      "[[3740   21]\n",
      " [  20 3906]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.012151778893396662\n",
      "Test Acc:  0.9966666666666667\n",
      "Test F1:  0.9966666296292181\n",
      "Test Precision:  0.9966887417218543\n",
      "Test Recall:  0.9966666666666666\n",
      "[[596   4]\n",
      " [  0 600]]\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.015467103774305742\n",
      "Acc:  0.9962273969038636\n",
      "F1:  0.9962255940795519\n",
      "Precision:  0.9962342150608099\n",
      "Recall:  0.9962172770029107\n",
      "[[3745   16]\n",
      " [  13 3913]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.020818056313192012\n",
      "Test Acc:  0.9958333333333333\n",
      "Test F1:  0.9958333072915039\n",
      "Test Precision:  0.9958457294765702\n",
      "Test Recall:  0.9958333333333333\n",
      "[[599   1]\n",
      " [  4 596]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.017284063049456823\n",
      "Acc:  0.995576948094185\n",
      "F1:  0.9955748595376255\n",
      "Precision:  0.9955805320455617\n",
      "Recall:  0.9955693220077957\n",
      "[[3743   18]\n",
      " [  16 3910]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.17644177818966758\n",
      "Test Acc:  0.9366666666666666\n",
      "Test F1:  0.9364116065551824\n",
      "Test Precision:  0.9437869822485208\n",
      "Test Recall:  0.9366666666666666\n",
      "[[524  76]\n",
      " [  0 600]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.020864861851405445\n",
      "Acc:  0.9941459607128919\n",
      "F1:  0.9941430961908257\n",
      "Precision:  0.9941577407293924\n",
      "Recall:  0.9941292940944295\n",
      "[[3736   25]\n",
      " [  20 3906]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.41646818754159387\n",
      "Test Acc:  0.7008333333333333\n",
      "Test F1:  0.6714257058283801\n",
      "Test Precision:  0.8128258602711158\n",
      "Test Recall:  0.7008333333333333\n",
      "[[241 359]\n",
      " [  0 600]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.01551408355324278\n",
      "Acc:  0.9954468583322492\n",
      "F1:  0.9954445242375372\n",
      "Precision:  0.9954721649512466\n",
      "Recall:  0.9954196168061544\n",
      "[[3739   22]\n",
      " [  13 3913]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.024222605989285315\n",
      "Test Acc:  0.9925\n",
      "Test F1:  0.9924999531247072\n",
      "Test Precision:  0.9925123128078202\n",
      "Test Recall:  0.9924999999999999\n",
      "[[594   6]\n",
      " [  3 597]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.019032517581389775\n",
      "Acc:  0.9934955119032132\n",
      "F1:  0.9934925136563245\n",
      "Precision:  0.9934925136563245\n",
      "Recall:  0.9934925136563245\n",
      "[[3736   25]\n",
      " [  25 3901]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.24228759122663177\n",
      "Test Acc:  0.9275\n",
      "Test F1:  0.9271169082489837\n",
      "Test Precision:  0.9366812227074236\n",
      "Test Recall:  0.9275\n",
      "[[513  87]\n",
      " [  0 600]]\n"
     ]
    }
   ],
   "source": [
    "# 20 epoch  시험 \n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

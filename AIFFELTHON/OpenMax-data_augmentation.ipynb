{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511911d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  2 03:54:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.203.03   Driver Version: 450.203.03   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806a13",
   "metadata": {},
   "source": [
    "# 라이브러리 및 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64780487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/project/AIFFELTHON'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights/om_weights')\n",
    "DATA_PATH = os.path.join('data')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # connected to GPU if 'cuda' is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fa817",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking imgs in a folder\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # prints file names\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # prints img info\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        if i==4:\n",
    "            break # print 4 per folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57610f",
   "metadata": {},
   "source": [
    "# Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9750162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize imgs, resize to 224x224\n",
    "# Create pipeline\n",
    "# PyTorch offers various augmentation techniques in torchvision.transforms.Compose\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b72fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'augmented_07_inner_cupholder': 0, 'resized_data12_inner_rear_seat': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating train dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43742791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train : 0\n",
      "data/train/resized_data12_inner_rear_seat : 1957\n",
      "data/train/augmented_07_inner_cupholder : 22000\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b699b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "test:  {'augmented_07_inner_cupholder': 0, 'resized_data12_inner_rear_seat': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating test dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "target_class_num = len(os.listdir(os.path.join(TEST_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0ec216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test : 0\n",
      "data/test/resized_data12_inner_rear_seat : 1957\n",
      "data/test/augmented_07_inner_cupholder : 22000\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "228b6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics from sklearn.metrics\n",
    "\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d5e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d38a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ad70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save best params based on acc\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'om_da_classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3cfda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa95ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /aiffel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e04e99bc2d243c3aa2234dd61fe03a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained resnet50\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02c5611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.06443814971546331\n",
      "Acc:  0.9795884292691072\n",
      "F1:  0.9310264985145935\n",
      "Precision:  0.9371508161057297\n",
      "Recall:  0.9251085265016026\n",
      "[[21785   215]\n",
      " [  274  1683]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.32896467832219173\n",
      "Test Acc:  0.9183119756229912\n",
      "Test F1:  0.47870835781273796\n",
      "Test Precision:  0.4591559878114956\n",
      "Test Recall:  0.5\n",
      "[[22000     0]\n",
      " [ 1957     0]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.055648288301813106\n",
      "Acc:  0.9821346579287891\n",
      "F1:  0.9393288108025717\n",
      "Precision:  0.947897629002494\n",
      "Recall:  0.931150206717146\n",
      "[[21826   174]\n",
      " [  254  1703]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.4679808384426859\n",
      "Test Acc:  0.9183119756229912\n",
      "Test F1:  0.47870835781273796\n",
      "Test Precision:  0.4591559878114956\n",
      "Test Recall:  0.5\n",
      "[[22000     0]\n",
      " [ 1957     0]]\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03666753076086752\n",
      "Acc:  0.988145427223776\n",
      "F1:  0.9604189872660122\n",
      "Precision:  0.9612802537031638\n",
      "Recall:  0.9595616435174432\n",
      "[[21862   138]\n",
      " [  146  1811]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.43204521133771595\n",
      "Test Acc:  0.8544058104103185\n",
      "Test F1:  0.7212774763211649\n",
      "Test Precision:  0.6796533167614853\n",
      "Test Recall:  0.9204945068983137\n",
      "[[18513  3487]\n",
      " [    1  1956]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03387945132267972\n",
      "Acc:  0.9888550319322119\n",
      "F1:  0.9627622093149062\n",
      "Precision:  0.9639545125129672\n",
      "Recall:  0.9615773679565197\n",
      "[[21872   128]\n",
      " [  139  1818]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.025945107044316504\n",
      "Test Acc:  0.9930709187293901\n",
      "Test F1:  0.9774214893472741\n",
      "Test Precision:  0.9671070268555283\n",
      "Test Recall:  0.9883132345426673\n",
      "[[21868   132]\n",
      " [   34  1923]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03196487788592155\n",
      "Acc:  0.9889385148390867\n",
      "F1:  0.9630756267578228\n",
      "Precision:  0.9638331509904192\n",
      "Recall:  0.9623211199888512\n",
      "[[21871   129]\n",
      " [  136  1821]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.19615510454927693\n",
      "Test Acc:  0.9183119756229912\n",
      "Test F1:  0.47870835781273796\n",
      "Test Precision:  0.4591559878114956\n",
      "Test Recall:  0.5\n",
      "[[22000     0]\n",
      " [ 1957     0]]\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.031117554779630154\n",
      "Acc:  0.9899403097215845\n",
      "F1:  0.9664197209382464\n",
      "Precision:  0.9671827145908121\n",
      "Recall:  0.9656597644818136\n",
      "[[21883   117]\n",
      " [  124  1833]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.1061229716906159\n",
      "Test Acc:  0.9183119756229912\n",
      "Test F1:  0.47870835781273796\n",
      "Test Precision:  0.4591559878114956\n",
      "Test Recall:  0.5\n",
      "[[22000     0]\n",
      " [ 1957     0]]\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03499477818592762\n",
      "Acc:  0.9903159828025212\n",
      "F1:  0.96757541454636\n",
      "Precision:  0.9697770647325353\n",
      "Recall:  0.965398778278441\n",
      "[[21894   106]\n",
      " [  126  1831]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01809002343308126\n",
      "Test Acc:  0.993864006344701\n",
      "Test F1:  0.9795840616015092\n",
      "Test Precision:  0.9788057332258004\n",
      "Test Recall:  0.9803654828819622\n",
      "[[21923    77]\n",
      " [   70  1887]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.028915849477518348\n",
      "Acc:  0.9899403097215845\n",
      "F1:  0.9664666392679444\n",
      "Precision:  0.9665752783685515\n",
      "Recall:  0.9663580619686905\n",
      "[[21880   120]\n",
      " [  121  1836]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.28118192055134533\n",
      "Test Acc:  0.9183119756229912\n",
      "Test F1:  0.47870835781273796\n",
      "Test Precision:  0.4591559878114956\n",
      "Test Recall:  0.5\n",
      "[[22000     0]\n",
      " [ 1957     0]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.027699351727652054\n",
      "Acc:  0.9911925533247068\n",
      "F1:  0.9707498388541664\n",
      "Precision:  0.9691200920229126\n",
      "Recall:  0.9723934942165653\n",
      "[[21887   113]\n",
      " [   98  1859]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.020057014502606222\n",
      "Test Acc:  0.9926535041950161\n",
      "Test F1:  0.9762441055246809\n",
      "Test Precision:  0.9626372403882366\n",
      "Test Recall:  0.9908791517629024\n",
      "[[21846   154]\n",
      " [   22  1935]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02822938190602387\n",
      "Acc:  0.9910255875109572\n",
      "F1:  0.9700005031605138\n",
      "Precision:  0.9714332547415397\n",
      "Recall:  0.9685783318623125\n",
      "[[21899   101]\n",
      " [  114  1843]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.05551538653746328\n",
      "Test Acc:  0.9792962390950453\n",
      "Test F1:  0.9227860553134043\n",
      "Test Precision:  0.9821351774150302\n",
      "Test Recall:  0.8783962698007154\n",
      "[[21978    22]\n",
      " [  474  1483]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026489024059847\n",
      "Acc:  0.9917351922193931\n",
      "F1:  0.9725328808472795\n",
      "Precision:  0.971221929261383\n",
      "Recall:  0.9738527779068147\n",
      "[[21895   105]\n",
      " [   93  1864]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.12519575424201002\n",
      "Test Acc:  0.9581333222022791\n",
      "Test F1:  0.8862613559562348\n",
      "Test Precision:  0.8306357391061796\n",
      "Test Recall:  0.9767390137966274\n",
      "[[20999  1001]\n",
      " [    2  1955]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.024281238405499606\n",
      "Acc:  0.9921943482072045\n",
      "F1:  0.9740045461938771\n",
      "Precision:  0.9736741118686871\n",
      "Recall:  0.9743355437357737\n",
      "[[21905    95]\n",
      " [   92  1865]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.016487401633529762\n",
      "Test Acc:  0.9950327670409483\n",
      "Test F1:  0.9834958124680089\n",
      "Test Precision:  0.982043473312218\n",
      "Test Recall:  0.9849588656106285\n",
      "[[21934    66]\n",
      " [   53  1904]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026430690261845788\n",
      "Acc:  0.9919438994865801\n",
      "F1:  0.9731704674621298\n",
      "Precision:  0.9728406146292676\n",
      "Recall:  0.9735008826125331\n",
      "[[21902    98]\n",
      " [   95  1862]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.03030092135005857\n",
      "Test Acc:  0.9899820511750219\n",
      "Test F1:  0.9679361362033639\n",
      "Test Precision:  0.9503435718031156\n",
      "Test Recall:  0.9873297138477262\n",
      "[[21791   209]\n",
      " [   31  1926]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02854778609999145\n",
      "Acc:  0.9909838460575198\n",
      "F1:  0.9700219607875852\n",
      "Precision:  0.9689340812815734\n",
      "Recall:  0.971116028708134\n",
      "[[21887   113]\n",
      " [  103  1854]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.0376284033954394\n",
      "Test Acc:  0.9890637391993989\n",
      "Test F1:  0.9655005933687715\n",
      "Test Precision:  0.9423192849963149\n",
      "Test Recall:  0.9919505620848237\n",
      "[[21747   253]\n",
      " [    9  1948]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02726528701481099\n",
      "Acc:  0.9921943482072045\n",
      "F1:  0.9739924485105997\n",
      "Precision:  0.9738821817557777\n",
      "Recall:  0.9741027779068147\n",
      "[[21906    94]\n",
      " [   93  1864]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.04920604473790378\n",
      "Test Acc:  0.9835956087990984\n",
      "Test F1:  0.9401099584633463\n",
      "Test Precision:  0.9879943172014436\n",
      "Test Recall:  0.9021516351558508\n",
      "[[21989    11]\n",
      " [  382  1575]]\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026238576511697222\n",
      "Acc:  0.9917351922193931\n",
      "F1:  0.9724561829330609\n",
      "Precision:  0.9724561829330609\n",
      "Recall:  0.9724561829330609\n",
      "[[21901    99]\n",
      " [   99  1858]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.030940069290914557\n",
      "Test Acc:  0.9882289101306507\n",
      "Test F1:  0.9589062958480716\n",
      "Test Precision:  0.9821594610634803\n",
      "Test Recall:  0.9381926417986715\n",
      "[[21956    44]\n",
      " [  238  1719]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02292972349980846\n",
      "Acc:  0.9927787285553283\n",
      "F1:  0.975905892691268\n",
      "Precision:  0.9764613388909376\n",
      "Recall:  0.9753520230408325\n",
      "[[21916    84]\n",
      " [   89  1868]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.019293921568745608\n",
      "Test Acc:  0.9941144550653254\n",
      "Test F1:  0.9799312491659773\n",
      "Test Precision:  0.9913933482918179\n",
      "Test Recall:  0.9690963208993357\n",
      "[[21978    22]\n",
      " [  119  1838]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026240946588184065\n",
      "Acc:  0.9914430020453312\n",
      "F1:  0.9714757719914051\n",
      "Precision:  0.9715855776773114\n",
      "Recall:  0.9713660287081339\n",
      "[[21898   102]\n",
      " [  103  1854]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.04785853470026489\n",
      "Test Acc:  0.9824685895562884\n",
      "Test F1:  0.9362766009728675\n",
      "Test Precision:  0.9810506308321535\n",
      "Test Recall:  0.9003741696474196\n",
      "[[21967    33]\n",
      " [  387  1570]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.029108819752465934\n",
      "Acc:  0.9905664315231456\n",
      "F1:  0.9684139814115403\n",
      "Precision:  0.9706195780153866\n",
      "Recall:  0.9662334394016816\n",
      "[[21897   103]\n",
      " [  123  1834]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.02024815256939027\n",
      "Test Acc:  0.993154401636265\n",
      "Test F1:  0.9774790768741164\n",
      "Test Precision:  0.9714471909619917\n",
      "Test Recall:  0.9837033725089422\n",
      "[[21890   110]\n",
      " [   54  1903]]\n",
      "> epoch:  19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.028553465272455166\n",
      "Acc:  0.9898150853612723\n",
      "F1:  0.9659937777919259\n",
      "Precision:  0.9668654702548576\n",
      "Recall:  0.9651260510057138\n",
      "[[21882   118]\n",
      " [  126  1831]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.06852488154580634\n",
      "Test Acc:  0.9829277455440999\n",
      "Test F1:  0.9429311897332564\n",
      "Test Precision:  0.9442814515328484\n",
      "Test Recall:  0.9415909555442004\n",
      "[[21802   198]\n",
      " [  211  1746]]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867d7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confidence function\n",
    "# need softmax and entropy for to check confidence\n",
    "# get the highest softmax value out of all softmax values and compute entropy based on the mathematical expression\n",
    "\n",
    "def get_confidence(net, infer_loader, device):    \n",
    "    container = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(infer_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            out = net(img) \n",
    "            out_softmax = torch.softmax(out, 1)\n",
    "\n",
    "            msp = float(out_softmax.detach().cpu().numpy().max()) # max softmax value\n",
    "\n",
    "            pA = out_softmax.detach().cpu().numpy() / out_softmax.detach().cpu().numpy().sum()\n",
    "            entropy = -np.sum( pA * np.log2(pA))\n",
    "\n",
    "            fname, _ = infer_loader.dataset.samples[idx]\n",
    "            label = int(label.detach().cpu().numpy())\n",
    "\n",
    "            tmp_container = {\n",
    "                'fname':fname,\n",
    "                'label':label,\n",
    "                'msp':msp,\n",
    "                'entropy':entropy\n",
    "            }\n",
    "            container.append(tmp_container)\n",
    "        \n",
    "    return container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2b45a1b",
   "metadata": {},
   "source": [
    "# Extract Activation Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1268ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need correct activation vector for openmax\n",
    "# input val in softmax layer is Activation Vector, so retrieve activation from torch.softmax()\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TRAIN_PATH))\n",
    "\n",
    "train_preds = list()\n",
    "train_actvecs = list()\n",
    "train_outputs_softmax = list()\n",
    "train_labels = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (img, label) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        out = net(img)\n",
    "        out_actvec = out.cpu().detach().numpy()[0]\n",
    "        out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "        out_pred = int(torch.argmax(out).cpu().detach().numpy())\n",
    "        out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "        train_actvecs.append(out_actvec) # component 1: Activation Vector before softmax\n",
    "        train_preds.append(out_pred) # componenet 2: preds of each data\n",
    "        train_outputs_softmax.append(out_softmax) # component 3: softmax of each data\n",
    "        train_labels.append(out_label) # component 4: labels of each data\n",
    "\n",
    "train_actvecs = np.asarray(train_actvecs)\n",
    "train_preds = np.asarray(train_preds)\n",
    "train_outputs_softmax = np.asarray(train_outputs_softmax)\n",
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df90a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation vector:  (23548, 2)\n",
      "Labels:  (23548,)\n"
     ]
    }
   ],
   "source": [
    "# only using correct activations vectors in OpenMax algorithm\n",
    "\n",
    "train_correct_actvecs = train_actvecs[train_labels==train_preds]\n",
    "train_correct_labels = train_labels[train_labels==train_preds]\n",
    "print('Activation vector: ', train_correct_actvecs.shape)\n",
    "print('Labels: ', train_correct_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "777b7b77",
   "metadata": {},
   "source": [
    "# Weibull-Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0697fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dce9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_idx:  0\n",
      "(21802, 2)\n",
      "class_idx:  1\n",
      "(1746, 2)\n"
     ]
    }
   ],
   "source": [
    "# parameters for weibull-dist are 3 = shape, loc, scale, class has 4 parameters, so total of 12 nums\n",
    "\n",
    "class_means = list()\n",
    "dist_to_means = list()\n",
    "mr_models = {}\n",
    "\n",
    "for class_idx in np.unique(train_labels):\n",
    "    \n",
    "    print('class_idx: ', class_idx)\n",
    "    class_act_vec = train_correct_actvecs[train_correct_labels==class_idx]\n",
    "    print(class_act_vec.shape)\n",
    "    \n",
    "    class_mean = class_act_vec.mean(axis=0)\n",
    "    class_means.append(class_mean)\n",
    "    \n",
    "    dist_to_mean = np.square(class_act_vec - class_mean).sum(axis=1) # compute distance of activation vectors\n",
    "    dist_to_mean_sorted = np.sort(dist_to_mean).astype(np.float64) # sort based on distance\n",
    "    dist_to_means.append(dist_to_mean_sorted)\n",
    "\n",
    "    shape, loc, scale = stats.weibull_max.fit(dist_to_mean_sorted[-100:]) # parameters of furthest 100 act vecs\n",
    "    \n",
    "    mr_models[str(class_idx)] = {\n",
    "        'shape':shape,\n",
    "        'loc':loc,\n",
    "        'scale':scale\n",
    "    }\n",
    "    \n",
    "class_means = np.asarray(class_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cff6a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_openmax(actvec, class_means, mr_models):\n",
    "    dist_to_mean = np.square(actvec - class_means).sum(axis=1)\n",
    "\n",
    "    scores = list()\n",
    "    for class_idx in range(len(class_means)):\n",
    "        params = mr_models[str(class_idx)]\n",
    "        score = stats.weibull_max.cdf(\n",
    "            dist_to_mean[class_idx],\n",
    "            params['shape'],\n",
    "            params['loc'],\n",
    "            params['scale']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    weight_on_actvec = 1 - scores # weight of each class\n",
    "    rev_actvec = np.concatenate([\n",
    "        weight_on_actvec * actvec, # multiplication of known class\n",
    "        [((1-weight_on_actvec) * actvec).sum()] # computing unknown class\n",
    "    ])\n",
    "    \n",
    "    openmax_prob = np.exp(rev_actvec) / np.exp(rev_actvec).sum()\n",
    "    return openmax_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce25d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(actvec, threshold, target_class_num, class_means, mr_models):\n",
    "    openmax_prob = compute_openmax(actvec, class_means, mr_models)\n",
    "    openmax_softmax = np.exp(openmax_prob)/sum(np.exp(openmax_prob))\n",
    "\n",
    "    pred = np.argmax(openmax_softmax)\n",
    "    if np.max(openmax_softmax) < threshold:\n",
    "        pred = target_class_num\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48bc515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_dataloader(net, data_loader, threshold, target_class_num, class_means, mr_models, is_reject=False):\n",
    "    result_preds = list()\n",
    "    result_labels = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(data_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "            out_actvec = out.cpu().detach().numpy()[0]\n",
    "            out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "            out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "            pred = inference(out_actvec, threshold, target_class_num, class_means, mr_models)\n",
    "\n",
    "            result_preds.append(pred)\n",
    "            if is_reject:\n",
    "                result_labels.append(target_class_num) # 3\n",
    "            else:\n",
    "                result_labels.append(out_label) # 0, 1, 2\n",
    "\n",
    "    return result_preds, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb608464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.1\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.2\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.30000000000000004\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.4\n",
      "Test Accuracy:  0.9721584505572484\n",
      "Reject Accuracy:  0.061224489795918366\n",
      "threshold:  0.5\n",
      "Test Accuracy:  0.9426472429770004\n",
      "Reject Accuracy:  0.15213358070500926\n",
      "threshold:  0.6\n",
      "Test Accuracy:  0.0\n",
      "Reject Accuracy:  1.0\n",
      "threshold:  0.7000000000000001\n",
      "Test Accuracy:  0.0\n",
      "Reject Accuracy:  1.0\n",
      "threshold:  0.8\n",
      "Test Accuracy:  0.0\n",
      "Reject Accuracy:  1.0\n",
      "threshold:  0.9\n",
      "Test Accuracy:  0.0\n",
      "Reject Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# finding right threshold\n",
    "for i in np.arange(0.1,1,0.1):\n",
    "    test_preds, test_labels = inference_dataloader(net, test_loader, i, target_class_num, class_means, mr_models)\n",
    "    reject_preds, reject_labels = inference_dataloader(net, reject_loader, i, target_class_num, class_means, mr_models, is_reject=True)\n",
    "    print('threshold: ', i)\n",
    "    print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "    print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6c94706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.3\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.31\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.32\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.33\n",
      "Test Accuracy:  0.982426848102851\n",
      "Reject Accuracy:  0.0018552875695732839\n",
      "threshold:  0.34\n",
      "Test Accuracy:  0.9812163459531661\n",
      "Reject Accuracy:  0.011131725417439703\n",
      "threshold:  0.35000000000000003\n",
      "Test Accuracy:  0.9790040489209835\n",
      "Reject Accuracy:  0.027829313543599257\n",
      "threshold:  0.36000000000000004\n",
      "Test Accuracy:  0.976207371540677\n",
      "Reject Accuracy:  0.03339517625231911\n",
      "threshold:  0.37000000000000005\n",
      "Test Accuracy:  0.9753725424719288\n",
      "Reject Accuracy:  0.04267161410018553\n",
      "threshold:  0.38000000000000006\n",
      "Test Accuracy:  0.9740368159619318\n",
      "Reject Accuracy:  0.05009276437847866\n",
      "threshold:  0.39000000000000007\n",
      "Test Accuracy:  0.9726593479984973\n",
      "Reject Accuracy:  0.055658627087198514\n",
      "threshold:  0.4000000000000001\n",
      "Test Accuracy:  0.9721584505572484\n",
      "Reject Accuracy:  0.061224489795918366\n",
      "threshold:  0.4100000000000001\n",
      "Test Accuracy:  0.969361773176942\n",
      "Reject Accuracy:  0.06679035250463822\n",
      "threshold:  0.4200000000000001\n",
      "Test Accuracy:  0.9673581834119463\n",
      "Reject Accuracy:  0.07421150278293136\n",
      "threshold:  0.4300000000000001\n",
      "Test Accuracy:  0.9661059398088241\n",
      "Reject Accuracy:  0.08534322820037106\n",
      "threshold:  0.4400000000000001\n",
      "Test Accuracy:  0.9636014526025796\n",
      "Reject Accuracy:  0.09090909090909091\n",
      "threshold:  0.4500000000000001\n",
      "Test Accuracy:  0.9611387068497725\n",
      "Reject Accuracy:  0.09833024118738404\n",
      "threshold:  0.46000000000000013\n",
      "Test Accuracy:  0.9604291021413366\n",
      "Reject Accuracy:  0.1038961038961039\n",
      "threshold:  0.47000000000000014\n",
      "Test Accuracy:  0.9571315273197812\n",
      "Reject Accuracy:  0.1150278293135436\n",
      "threshold:  0.48000000000000015\n",
      "Test Accuracy:  0.9524564845347915\n",
      "Reject Accuracy:  0.12430426716141002\n",
      "threshold:  0.49000000000000016\n",
      "Test Accuracy:  0.9484493050048003\n",
      "Reject Accuracy:  0.1391465677179963\n"
     ]
    }
   ],
   "source": [
    "# finding right threshold\n",
    "for i in np.arange(0.3,0.5,0.01):\n",
    "    test_preds, test_labels = inference_dataloader(net, test_loader, i, target_class_num, class_means, mr_models)\n",
    "    reject_preds, reject_labels = inference_dataloader(net, reject_loader, i, target_class_num, class_means, mr_models, is_reject=True)\n",
    "    print('threshold: ', i)\n",
    "    print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "    print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

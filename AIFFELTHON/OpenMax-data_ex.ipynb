{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaaba846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/0_felton'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data_ex')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # 여기서 'cuda'가 출력되어야 GPU와 연결이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train path image 확인 각폴더당 1개\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # 파일이름을 출력합니다\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # 이미지 정보를 출력합니다\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        break # 폴더마다 1장만 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject foler image 확인 5개\n",
    "for dirpath, dirnames, filenames in os.walk(REJECT_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if i > 4:\n",
    "            break\n",
    "        print(os.path.join(dirpath, filename))\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})')\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3ecab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 전처리 함수 만들기 resize 224, 224 -> resNet 사용 shape조절\n",
    "\n",
    "# train data image 증강\n",
    "# random H-flip, V-flip image 증강\n",
    "# 색깔 바꾸기 colorjitter \n",
    "# center crop 사용안함\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "#         torchvision.transforms.CenterCrop((224,224)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "#         torchvision.transforms.CenterCrop((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  3\n",
      "train:  {'07_inner_cupholder_resized1': 0, '07_inner_cupholder_resized2': 1, '07_inner_cupholder_resized3': 2}\n"
     ]
    }
   ],
   "source": [
    "# 위에서 만든 함수로 데이터셋 준비\n",
    "# train dataset\n",
    "# batch_size = 32\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78dd50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex/train : 0\n",
      "/aiffel/aiffel/0_felton/data_ex/train/07_inner_cupholder_resized1 : 500\n",
      "/aiffel/aiffel/0_felton/data_ex/train/07_inner_cupholder_resized2 : 500\n",
      "/aiffel/aiffel/0_felton/data_ex/train/07_inner_cupholder_resized3 : 500\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스별 이미지 개수 확인\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{dirpath} : {len(filenames)}')\n",
    "    \n",
    "# 각각 500개씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53dc9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  {'07_inner_cupholder_resized4': 0, '07_inner_cupholder_resized5': 1, '07_inner_cupholder_resized6': 2}\n"
     ]
    }
   ],
   "source": [
    "# test data set 준비\n",
    "# shuffle = False\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd54514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex/test : 0\n",
      "/aiffel/aiffel/0_felton/data_ex/test/07_inner_cupholder_resized4 : 167\n",
      "/aiffel/aiffel/0_felton/data_ex/test/07_inner_cupholder_resized6 : 167\n",
      "/aiffel/aiffel/0_felton/data_ex/test/07_inner_cupholder_resized5 : 166\n"
     ]
    }
   ],
   "source": [
    "# 개수 확인\n",
    "for dirpath, dirnames, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{dirpath} : {len(filenames)}')\n",
    "# 167,167,166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb14b6",
   "metadata": {},
   "source": [
    "### model 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf66fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 함수\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5479c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b59c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 함수\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc858a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 함수\n",
    "# train -> test -> save 반복\n",
    "# 가장 좋은 accuracy 를 저장\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f9488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /aiffel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d38f805da42efb06543174b5db9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 가져오기\n",
    "# 사용모델 resNet 50, pretrained = True\n",
    "# pretraine 된 모델로 전이 학습예정\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e0e19",
   "metadata": {},
   "source": [
    "### 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca01fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.3779333588924814\n",
      "Acc:  0.34933333333333333\n",
      "F1:  0.34913334031065785\n",
      "Precision:  0.3504434949855215\n",
      "Recall:  0.34933333333333333\n",
      "[[192 177 131]\n",
      " [203 150 147]\n",
      " [171 147 182]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  3.318859286114517\n",
      "Test Acc:  0.3\n",
      "Test F1:  0.2161336925047157\n",
      "Test Precision:  0.18631695102283338\n",
      "Test Recall:  0.2994011976047904\n",
      "[[120   0  47]\n",
      " [117   0  49]\n",
      " [137   0  30]]\n",
      "[Notification] Best Model Updated!\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch  시험 \n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe58fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.1417904483511092\n",
      "Acc:  0.35733333333333334\n",
      "F1:  0.3515060040248957\n",
      "Precision:  0.35843297609830405\n",
      "Recall:  0.35733333333333334\n",
      "[[239 124 137]\n",
      " [241 124 135]\n",
      " [211 116 173]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.1110698628425597\n",
      "Test Acc:  0.332\n",
      "Test F1:  0.16616616616616617\n",
      "Test Precision:  0.11066666666666668\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0 167   0]\n",
      " [  0 166   0]\n",
      " [  0 167   0]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0888753520681502\n",
      "Acc:  0.4033333333333333\n",
      "F1:  0.393612801828401\n",
      "Precision:  0.39891739483572897\n",
      "Recall:  0.4033333333333333\n",
      "[[267 117 116]\n",
      " [261 111 128]\n",
      " [163 110 227]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.397289239168167\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[167   0   0]\n",
      " [166   0   0]\n",
      " [167   0   0]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0802828682229875\n",
      "Acc:  0.4066666666666667\n",
      "F1:  0.396183643177663\n",
      "Precision:  0.40126503126503127\n",
      "Recall:  0.4066666666666667\n",
      "[[110 242 148]\n",
      " [121 251 128]\n",
      " [ 84 167 249]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.3251803961992263\n",
      "Test Acc:  0.332\n",
      "Test F1:  0.16641604010025063\n",
      "Test Precision:  0.1111111111111111\n",
      "Test Recall:  0.3313373253493014\n",
      "[[166   0   1]\n",
      " [165   0   1]\n",
      " [167   0   0]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0659315497317212\n",
      "Acc:  0.428\n",
      "F1:  0.42698543902227265\n",
      "Precision:  0.42804963422588327\n",
      "Recall:  0.428\n",
      "[[220 168 112]\n",
      " [224 160 116]\n",
      " [125 113 262]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.2314568035602569\n",
      "Test Acc:  0.332\n",
      "Test F1:  0.16616616616616617\n",
      "Test Precision:  0.11066666666666668\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0 167   0]\n",
      " [  0 166   0]\n",
      " [  0 167   0]]\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0717671868648935\n",
      "Acc:  0.41\n",
      "F1:  0.4064979134516504\n",
      "Precision:  0.4058966147388228\n",
      "Recall:  0.41\n",
      "[[208 158 134]\n",
      " [217 145 138]\n",
      " [127 111 262]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  2.1612446783185004\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0   0 167]\n",
      " [  0   0 166]\n",
      " [  0   0 167]]\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0576261497558432\n",
      "Acc:  0.43\n",
      "F1:  0.42589781787858255\n",
      "Precision:  0.4250815295767396\n",
      "Recall:  0.43\n",
      "[[198 169 133]\n",
      " [209 158 133]\n",
      " [124  87 289]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.1183533989191055\n",
      "Test Acc:  0.314\n",
      "Test F1:  0.24146934039235593\n",
      "Test Precision:  0.20686274509803923\n",
      "Test Recall:  0.31468388524156504\n",
      "[[  0 112  55]\n",
      " [  0 109  57]\n",
      " [  0 119  48]]\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0483908488395366\n",
      "Acc:  0.432\n",
      "F1:  0.43099705268509636\n",
      "Precision:  0.4301531345319094\n",
      "Recall:  0.432\n",
      "[[187 195 118]\n",
      " [191 185 124]\n",
      " [116 108 276]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.1750889911651612\n",
      "Test Acc:  0.298\n",
      "Test F1:  0.1641047259933538\n",
      "Test Precision:  0.13266283524904213\n",
      "Test Recall:  0.2974051896207585\n",
      "[[146   0  21]\n",
      " [154   0  12]\n",
      " [164   0   3]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.047174463880823\n",
      "Acc:  0.44266666666666665\n",
      "F1:  0.43665171644914436\n",
      "Precision:  0.43685880387491816\n",
      "Recall:  0.4426666666666666\n",
      "[[218 147 135]\n",
      " [225 148 127]\n",
      " [116  86 298]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  10.052291729007038\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0   0 167]\n",
      " [  0   0 166]\n",
      " [  0   0 167]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0349264461943444\n",
      "Acc:  0.44733333333333336\n",
      "F1:  0.44242204351599307\n",
      "Precision:  0.4426138338547097\n",
      "Recall:  0.44733333333333336\n",
      "[[154 231 115]\n",
      " [152 216 132]\n",
      " [ 86 113 301]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.2738091692030429\n",
      "Test Acc:  0.292\n",
      "Test F1:  0.28367429098538294\n",
      "Test Precision:  0.29546078004227777\n",
      "Test Recall:  0.29224683163792897\n",
      "[[29 65 73]\n",
      " [36 69 61]\n",
      " [26 93 48]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.028036524640753\n",
      "Acc:  0.4573333333333333\n",
      "F1:  0.45715357137069224\n",
      "Precision:  0.45833689290886853\n",
      "Recall:  0.4573333333333333\n",
      "[[180 233  87]\n",
      " [171 213 116]\n",
      " [100 107 293]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.221249547123909\n",
      "Test Acc:  0.31\n",
      "Test F1:  0.24654970760233919\n",
      "Test Precision:  0.20618153364632238\n",
      "Test Recall:  0.31049948296178725\n",
      "[[ 62 103   2]\n",
      " [ 72  93   1]\n",
      " [ 79  88   0]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0196142120564238\n",
      "Acc:  0.45\n",
      "F1:  0.4237586268019071\n",
      "Precision:  0.4309615633912367\n",
      "Recall:  0.45\n",
      "[[299 103  98]\n",
      " [294  74 132]\n",
      " [143  55 302]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.1415514237880706\n",
      "Test Acc:  0.298\n",
      "Test F1:  0.21794487702716217\n",
      "Test Precision:  0.2669465605417209\n",
      "Test Recall:  0.29883606281413083\n",
      "[[  1 113  53]\n",
      " [  2 119  45]\n",
      " [  1 137  29]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0425706264820505\n",
      "Acc:  0.43466666666666665\n",
      "F1:  0.43273326971151066\n",
      "Precision:  0.4318672887274686\n",
      "Recall:  0.4346666666666667\n",
      "[[205 176 119]\n",
      " [206 168 126]\n",
      " [116 105 279]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.200711988210678\n",
      "Test Acc:  0.304\n",
      "Test F1:  0.17742522599041435\n",
      "Test Precision:  0.15807895807895808\n",
      "Test Recall:  0.30339321357285426\n",
      "[[145   0  22]\n",
      " [150   0  16]\n",
      " [160   0   7]]\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0328164874239172\n",
      "Acc:  0.43666666666666665\n",
      "F1:  0.43367307551722395\n",
      "Precision:  0.43390718942250434\n",
      "Recall:  0.4366666666666667\n",
      "[[232 165 103]\n",
      " [228 148 124]\n",
      " [119 106 275]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.2331452490091324\n",
      "Test Acc:  0.324\n",
      "Test F1:  0.20538029386343995\n",
      "Test Precision:  0.22650375939849623\n",
      "Test Recall:  0.3235336555804055\n",
      "[[147  19   1]\n",
      " [150  15   1]\n",
      " [159   8   0]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0369691214662917\n",
      "Acc:  0.44066666666666665\n",
      "F1:  0.41706658320582374\n",
      "Precision:  0.43179258317427877\n",
      "Recall:  0.4406666666666667\n",
      "[[302  87 111]\n",
      " [302  80 118]\n",
      " [160  61 279]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.1760276601314545\n",
      "Test Acc:  0.29\n",
      "Test F1:  0.23021228836831675\n",
      "Test Precision:  0.23218620606090992\n",
      "Test Recall:  0.28990212346391553\n",
      "[[103  55   9]\n",
      " [116  40  10]\n",
      " [129  36   2]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0264344291484102\n",
      "Acc:  0.436\n",
      "F1:  0.43349886423314693\n",
      "Precision:  0.4339868955298038\n",
      "Recall:  0.43599999999999994\n",
      "[[161 233 106]\n",
      " [170 201 129]\n",
      " [ 85 123 292]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.2762369492650032\n",
      "Test Acc:  0.348\n",
      "Test F1:  0.2698427260812582\n",
      "Test Precision:  0.2867912159947558\n",
      "Test Recall:  0.347317413366039\n",
      "[[ 47   6 114]\n",
      " [ 27   1 138]\n",
      " [ 39   2 126]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0224250516992934\n",
      "Acc:  0.4666666666666667\n",
      "F1:  0.46291955171462024\n",
      "Precision:  0.4616727685332556\n",
      "Recall:  0.4666666666666666\n",
      "[[219 169 112]\n",
      " [209 171 120]\n",
      " [104  86 310]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.3652014828920365\n",
      "Test Acc:  0.324\n",
      "Test F1:  0.18823658209486274\n",
      "Test Precision:  0.21940928270042193\n",
      "Test Recall:  0.32344948656903066\n",
      "[[154  13   0]\n",
      " [156   8   2]\n",
      " [164   3   0]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.012955812697715\n",
      "Acc:  0.4613333333333333\n",
      "F1:  0.4553782490606766\n",
      "Precision:  0.4587633210319592\n",
      "Recall:  0.4613333333333333\n",
      "[[251 151  98]\n",
      " [250 140 110]\n",
      " [126  73 301]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  3.4740499131542166\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0   0 167]\n",
      " [  0   0 166]\n",
      " [  0   0 167]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0240542584277215\n",
      "Acc:  0.45266666666666666\n",
      "F1:  0.44813871805022254\n",
      "Precision:  0.4489956594115325\n",
      "Recall:  0.4526666666666667\n",
      "[[229 172  99]\n",
      " [244 144 112]\n",
      " [119  75 306]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.4897668959498405\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[  0   0 167]\n",
      " [  0   0 166]\n",
      " [  0   0 167]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0140222919748185\n",
      "Acc:  0.464\n",
      "F1:  0.4574588247363611\n",
      "Precision:  0.45867777008228616\n",
      "Recall:  0.46399999999999997\n",
      "[[246 163  91]\n",
      " [238 140 122]\n",
      " [122  68 310]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test Metrics ======\n",
      "Test Loss:  1.200905968248844\n",
      "Test Acc:  0.272\n",
      "Test F1:  0.25159442987338904\n",
      "Test Precision:  0.26232560405003486\n",
      "Test Recall:  0.27179376187384263\n",
      "[[ 83  38  46]\n",
      " [ 96  28  42]\n",
      " [106  36  25]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  1.0264040789705642\n",
      "Acc:  0.45866666666666667\n",
      "F1:  0.45189459767824536\n",
      "Precision:  0.4549601480953545\n",
      "Recall:  0.4586666666666666\n",
      "[[274 145  81]\n",
      " [251 132 117]\n",
      " [122  96 282]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.4902878039479255\n",
      "Test Acc:  0.334\n",
      "Test F1:  0.16691654172913542\n",
      "Test Precision:  0.11133333333333334\n",
      "Test Recall:  0.3333333333333333\n",
      "[[167   0   0]\n",
      " [166   0   0]\n",
      " [167   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 20 epoch  시험 \n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fbb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded the Network Weight!\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 불러오기 20회 학습\n",
    "saved_weight_path = os.path.join(MODEL_PATH, 'classifier_acc_0.34800.pth')\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.load_state_dict(torch.load(saved_weight_path, map_location=device))\n",
    "net.eval()\n",
    "net.to(device)    \n",
    "\n",
    "print('Successfully Loaded the Network Weight!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e59e83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence 확인할 함수\n",
    "\n",
    "def get_confidence(net, infer_loader, device):    \n",
    "    container = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(infer_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            out = net(img) \n",
    "            out_softmax = torch.softmax(out, 1)\n",
    "\n",
    "            msp = float(out_softmax.detach().cpu().numpy().max()) # 최대 softmax 값\n",
    "\n",
    "            pA = out_softmax.detach().cpu().numpy() / out_softmax.detach().cpu().numpy().sum()\n",
    "            entropy = -np.sum( pA * np.log2(pA))\n",
    "\n",
    "            fname, _ = infer_loader.dataset.samples[idx]\n",
    "            label = int(label.detach().cpu().numpy())\n",
    "\n",
    "            tmp_container = {\n",
    "                'fname':fname,\n",
    "                'label':label,\n",
    "                'msp':msp,\n",
    "                'entropy':entropy\n",
    "            }\n",
    "            container.append(tmp_container)\n",
    "        \n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cee56b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  {'07_inner_cupholder_resized4': 0, '07_inner_cupholder_resized5': 1, '07_inner_cupholder_resized6': 2}\n",
      "{'fname': '/aiffel/aiffel/0_felton/data_ex/test/07_inner_cupholder_resized4/fulfillment-image_05╚г0421_2021.08.26_624926.png', 'label': 0, 'msp': 0.4830590784549713, 'entropy': 1.5153935}\n"
     ]
    }
   ],
   "source": [
    "# confidence 확인 test\n",
    "\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, 1, False)\n",
    "print('Test: ', _test_data.class_to_idx)\n",
    "\n",
    "test_result = get_confidence(net, test_loader, device)\n",
    "print(test_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "524edaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject:  {'08_inner_cupholder_dirt_300': 0}\n",
      "{'fname': '/aiffel/aiffel/0_felton/data_ex/reject/08_inner_cupholder_dirt_300/000100_QEXB4PjGXR-cCirpP89AlPrJ.jpeg', 'label': 0, 'msp': 0.49903401732444763, 'entropy': 1.5001444}\n"
     ]
    }
   ],
   "source": [
    "# confidence 확인 reject\n",
    "\n",
    "reject_loader, _reject_data = create_dataloader(REJECT_PATH, 1, False)\n",
    "print('Reject: ', _reject_data.class_to_idx)\n",
    "\n",
    "reject_result = get_confidence(net, reject_loader, device)\n",
    "print(reject_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f5b93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence 시각화\n",
    "# softmax와 entropy 따로 떼어내기\n",
    "\n",
    "def parse_result(results):\n",
    "    msp, entropy = list(), list()\n",
    "    for result in results:\n",
    "        msp.append(result['msp'])\n",
    "        entropy.append(result['entropy'])\n",
    "    return msp, entropy\n",
    "\n",
    "test_msp, test_entropy = parse_result(test_result)\n",
    "reject_msp, reject_entropy = parse_result(reject_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d22722ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# softmax분포 확인\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.histplot(test_msp, label='Test', color='orange')\n",
    "sns.histplot(reject_msp, label='Reject', color='teal')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8531befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#entropy 분포 확인\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.histplot(test_entropy, label='Test', color='orange')\n",
    "sns.histplot(reject_entropy, label='Reject', color='teal')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38afc3",
   "metadata": {},
   "source": [
    "### OpenMax 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "836eaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded the Network Weight!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data와 모델 가져오기\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TRAIN_PATH))\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "saved_weight_path = os.path.join(MODEL_PATH, 'classifier_acc_0.34800.pth')\n",
    "net.load_state_dict(torch.load(saved_weight_path, map_location=device))\n",
    "print('Successfully Loaded the Network Weight!')\n",
    "net.eval()\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca911279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation vector 뽑아오기\n",
    "\n",
    "train_preds = list()\n",
    "train_actvecs = list()\n",
    "train_outputs_softmax = list()\n",
    "train_labels = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx, (img, label) in enumerate(train_loader):\n",
    "      img = img.to(device)\n",
    "      label = label.to(device)\n",
    "\n",
    "      out = net(img)\n",
    "      out_actvec = out.cpu().detach().numpy()[0]\n",
    "      out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "      out_pred = int(torch.argmax(out).cpu().detach().numpy())\n",
    "      out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "      train_actvecs.append(out_actvec) # component 1: softmax 전의 Activation Vector\n",
    "      train_preds.append(out_pred) # componenet 2: 각 데이터에 대한 예측값\n",
    "      train_outputs_softmax.append(out_softmax) # component 3: 각 데이터에 대한 softmax 확률\n",
    "      train_labels.append(out_label) # component 4: 각 데이터에 대한 Label (정답)\n",
    "\n",
    "train_actvecs = np.asarray(train_actvecs)\n",
    "train_preds = np.asarray(train_preds)\n",
    "train_outputs_softmax = np.asarray(train_outputs_softmax)\n",
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3f21942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation vector:  (1500, 3)\n",
      "Labels:  (1500,)\n"
     ]
    }
   ],
   "source": [
    "# 정답을 맞힌 액티베이션 벡터만 사용\n",
    "# 어차피 전부 정답이니 사용 안함 테스트 시험\n",
    "# 다음번에 할때에는 정답을 하나 만들고 하나는 다른 클래스 만들어서 시험해보자\n",
    "\n",
    "# train_correct_actvecs = train_actvecs[train_labels==train_preds]\n",
    "# train_correct_labels = train_labels[train_labels==train_preds]\n",
    "# print('Activation vector: ', train_correct_actvecs.shape)\n",
    "# print('Labels: ', train_correct_labels.shape)\n",
    "\n",
    "# 그냥 모든 벡터 사용\n",
    "train_correct_actvecs = train_actvecs\n",
    "train_correct_labels = train_labels\n",
    "print('Activation vector: ', train_correct_actvecs.shape)\n",
    "print('Labels: ', train_correct_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cea0400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_idx:  0\n",
      "(500, 3)\n",
      "class_idx:  1\n",
      "(500, 3)\n",
      "class_idx:  2\n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "# 이상치의 weibull 분포 그래프 만들기\n",
    "class_means = list()\n",
    "dist_to_means = list()\n",
    "mr_models = {}\n",
    "\n",
    "for class_idx in np.unique(train_labels):\n",
    "    \n",
    "    print('class_idx: ', class_idx)\n",
    "    class_act_vec = train_correct_actvecs[train_correct_labels==class_idx]\n",
    "    print(class_act_vec.shape)\n",
    "    \n",
    "    class_mean = class_act_vec.mean(axis=0)\n",
    "    class_means.append(class_mean)\n",
    "    \n",
    "    dist_to_mean = np.square(class_act_vec - class_mean).sum(axis=1) # 각 activation vector의 거리를 계산\n",
    "    dist_to_mean_sorted = np.sort(dist_to_mean).astype(np.float64) # 거리를 기준으로 오름차순 정렬\n",
    "    dist_to_means.append(dist_to_mean_sorted)\n",
    "\n",
    "    shape, loc, scale = stats.weibull_max.fit(dist_to_mean_sorted[-100:]) # 거리가 가장 먼 100개를 사용하여 모수 추출\n",
    "    \n",
    "    mr_models[str(class_idx)] = {\n",
    "        'shape':shape,\n",
    "        'loc':loc,\n",
    "        'scale':scale\n",
    "    }\n",
    "    \n",
    "class_means = np.asarray(class_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73a6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오픈 맥스 적용\n",
    "\n",
    "def compute_openmax(actvec, class_means, mr_models):\n",
    "    dist_to_mean = np.square(actvec - class_means).sum(axis=1)\n",
    "\n",
    "    scores = list()\n",
    "    for class_idx in range(len(class_means)):\n",
    "        params = mr_models[str(class_idx)]\n",
    "        score = stats.weibull_max.cdf(\n",
    "            dist_to_mean[class_idx],\n",
    "            params['shape'],\n",
    "            params['loc'],\n",
    "            params['scale']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    weight_on_actvec = 1 - scores # 각 class별 가중치\n",
    "    rev_actvec = np.concatenate([\n",
    "        weight_on_actvec * actvec, # known class에 대한 가중치 곱\n",
    "        [((1-weight_on_actvec) * actvec).sum()] # unknown class에 새로운 계산식\n",
    "    ])\n",
    "    \n",
    "    openmax_prob = np.exp(rev_actvec) / np.exp(rev_actvec).sum()\n",
    "    return openmax_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9784cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference 함수\n",
    "def inference(actvec, threshold, target_class_num, class_means, mr_models):\n",
    "    openmax_prob = compute_openmax(actvec, class_means, mr_models)\n",
    "    openmax_softmax = np.exp(openmax_prob)/sum(np.exp(openmax_prob))\n",
    "\n",
    "    pred = np.argmax(openmax_softmax)\n",
    "    if np.max(openmax_softmax) < threshold:\n",
    "        pred = target_class_num\n",
    "    return pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eaf7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold 탐색\n",
    "\n",
    "def inference_dataloader(net, data_loader, threshold, target_class_num, class_means, mr_models, is_reject=False):\n",
    "    result_preds = list()\n",
    "    result_labels = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, (img, label) in enumerate(data_loader):\n",
    "          img = img.to(device)\n",
    "          label = label.to(device)\n",
    "\n",
    "          out = net(img)\n",
    "          out_actvec = out.cpu().detach().numpy()[0]\n",
    "          out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "          out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "          pred = inference(out_actvec, threshold, target_class_num, class_means, mr_models)\n",
    "      \n",
    "          result_preds.append(pred)\n",
    "          if is_reject:\n",
    "              result_labels.append(target_class_num)\n",
    "          else:\n",
    "              result_labels.append(out_label)\n",
    "\n",
    "    return result_preds, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaf18051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.06\n",
      "Reject Accuracy:  0.75139146567718\n"
     ]
    }
   ],
   "source": [
    "# threshold 0.35로 탐색\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, 1, False)\n",
    "reject_loader, _reject_data = create_dataloader(REJECT_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TEST_PATH))\n",
    "\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.35, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.35, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17235de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.006\n",
      "Reject Accuracy:  0.9814471243042672\n"
     ]
    }
   ],
   "source": [
    "# threshold 0.4\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.4, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "199ab408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.0\n",
      "Reject Accuracy:  0.9814471243042672\n"
     ]
    }
   ],
   "source": [
    "# threshold 0.5\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.5, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d165fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.036\n",
      "Reject Accuracy:  0.9814471243042672\n"
     ]
    }
   ],
   "source": [
    "# 0.37\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.37, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59ecf63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.334\n",
      "Reject Accuracy:  0.9814471243042672\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.2, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d2ab9d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.334\n",
      "Reject Accuracy:  0.9814471243042672\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.1, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c0b9edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.03\n",
      "Reject Accuracy:  0.8942486085343229\n"
     ]
    }
   ],
   "source": [
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.38, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.38, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee6638",
   "metadata": {},
   "source": [
    "### 회고\n",
    "- 더트 이미지에대한 분류가 생각 보다 잘된다\n",
    "- test 이미지는  원래가 분류가 어려워서 한쪽으로 몰렷기때문에 0.34 1/3 이 나온것으로 보인다.\n",
    "- train data 하나로 바꾸고 data 이외의 카테고리를 넣어줘서 실험해보자\n",
    "- 예를 들어 train 0 을 컵홀더 이미지 train 1을 실내 이미지 reject를 더러운 이미지로 넣으면 컵홀더를 분류를 잘해내는지 test accuracy와, 비슷한 이미지인 컵홀더와 컵홀더가 더러운 상태의 이미지를 구분 할수 있는지 reject accuracy로 확인이 가능할 것으로 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

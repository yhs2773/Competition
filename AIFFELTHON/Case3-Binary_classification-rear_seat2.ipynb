{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8bf0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/0_felton'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data_ex_rear2')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # connected to GPU if 'cuda' is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking imgs in a folder\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # prints file names\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # prints img info\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        break # print 1 per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed81261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize imgs, resize to 224x224\n",
    "# Create pipeline\n",
    "# PyTorch offers various augmentation techniques in torchvision.transforms.Compose\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f220a408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'12_inner_rear_seat_train_2000': 0, 'resized_14_inner_sheet_dirt_train': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating train dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264a347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_rear2/train : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_rear2/train/12_inner_rear_seat_train_2000 : 2000\n",
      "/aiffel/aiffel/0_felton/data_ex_rear2/train/resized_14_inner_sheet_dirt_train : 2000\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29294b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  {'12_inner_rear_seat_test_500': 0, 'resized_14_inner_sheet_dirt_test': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating test dataset\n",
    "# shuffle = False\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a56e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_rear2/test : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_rear2/test/resized_14_inner_sheet_dirt_test : 500\n",
      "/aiffel/aiffel/0_felton/data_ex_rear2/test/12_inner_rear_seat_test_500 : 500\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c1f6a",
   "metadata": {},
   "source": [
    "### model 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654281c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics from sklearn.metrics\n",
    "\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5266a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed56b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972992b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save best params based on acc\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8397ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained resnet50\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208885c4",
   "metadata": {},
   "source": [
    "### 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3eb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.3388406404554844\n",
      "Acc:  0.88075\n",
      "F1:  0.8805564940551502\n",
      "Precision:  0.8832334485549985\n",
      "Recall:  0.88075\n",
      "[[1681  319]\n",
      " [ 158 1842]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.15834427163157064\n",
      "Test Acc:  0.938\n",
      "Test F1:  0.9377907280090223\n",
      "Test Precision:  0.943974115700871\n",
      "Test Recall:  0.938\n",
      "[[440  60]\n",
      " [  2 498]]\n",
      "[Notification] Best Model Updated!\n"
     ]
    }
   ],
   "source": [
    "# test with 1 epoch\n",
    "\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5218f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.20420244591683148\n",
      "Acc:  0.9455\n",
      "F1:  0.945493404701969\n",
      "Precision:  0.9457157264115832\n",
      "Recall:  0.9455\n",
      "[[1869  131]\n",
      " [  87 1913]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08185703790359003\n",
      "Test Acc:  0.977\n",
      "Test F1:  0.9769898525249636\n",
      "Test Precision:  0.977842914901887\n",
      "Test Recall:  0.977\n",
      "[[478  22]\n",
      " [  1 499]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.09436291536688804\n",
      "Acc:  0.976\n",
      "F1:  0.9759997059963985\n",
      "Precision:  0.976023325142932\n",
      "Recall:  0.976\n",
      "[[1945   55]\n",
      " [  41 1959]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.7899331645962083\n",
      "Test Acc:  0.598\n",
      "Test F1:  0.5214285714285715\n",
      "Test Precision:  0.7722222222222221\n",
      "Test Recall:  0.598\n",
      "[[ 99 401]\n",
      " [  1 499]]\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.06037014590762556\n",
      "Acc:  0.983\n",
      "F1:  0.982999893749336\n",
      "Precision:  0.9830120753018825\n",
      "Recall:  0.9830000000000001\n",
      "[[1961   39]\n",
      " [  29 1971]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.34473994121979923\n",
      "Test Acc:  0.888\n",
      "Test F1:  0.8866282012349429\n",
      "Test Precision:  0.9077343421605717\n",
      "Test Recall:  0.888\n",
      "[[389 111]\n",
      " [  1 499]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.05070817659795284\n",
      "Acc:  0.98275\n",
      "F1:  0.9827498695458885\n",
      "Precision:  0.9827646036292599\n",
      "Recall:  0.98275\n",
      "[[1960   40]\n",
      " [  29 1971]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.032765346544985995\n",
      "Test Acc:  0.994\n",
      "Test F1:  0.9939999039984639\n",
      "Test Precision:  0.9940316180235536\n",
      "Test Recall:  0.994\n",
      "[[495   5]\n",
      " [  1 499]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03465004736464471\n",
      "Acc:  0.98975\n",
      "F1:  0.98974998398435\n",
      "Precision:  0.989753060956631\n",
      "Recall:  0.98975\n",
      "[[1977   23]\n",
      " [  18 1982]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.19873114029136535\n",
      "Test Acc:  0.971\n",
      "Test F1:  0.9709755904715865\n",
      "Test Precision:  0.9725897920604916\n",
      "Test Recall:  0.971\n",
      "[[500   0]\n",
      " [ 29 471]]\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.04520266409218311\n",
      "Acc:  0.98825\n",
      "F1:  0.9882498347633013\n",
      "Precision:  0.9882774656074405\n",
      "Recall:  0.9882500000000001\n",
      "[[1969   31]\n",
      " [  16 1984]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.06762628827503771\n",
      "Test Acc:  0.989\n",
      "Test F1:  0.9889986688389294\n",
      "Test Precision:  0.9892367906066536\n",
      "Test Recall:  0.989\n",
      "[[500   0]\n",
      " [ 11 489]]\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.037171187703497706\n",
      "Acc:  0.99075\n",
      "F1:  0.9907499716717882\n",
      "Precision:  0.990756011761144\n",
      "Recall:  0.99075\n",
      "[[1978   22]\n",
      " [  15 1985]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.06613717172491153\n",
      "Test Acc:  0.989\n",
      "Test F1:  0.9889991089278232\n",
      "Test Precision:  0.9891584873499013\n",
      "Test Recall:  0.989\n",
      "[[499   1]\n",
      " [ 10 490]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.04346495474223047\n",
      "Acc:  0.98975\n",
      "F1:  0.9897499686092789\n",
      "Precision:  0.989755999510994\n",
      "Recall:  0.98975\n",
      "[[1976   24]\n",
      " [  17 1983]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.006778125748967461\n",
      "Test Acc:  0.999\n",
      "Test F1:  0.9989999989999989\n",
      "Test Precision:  0.999001996007984\n",
      "Test Recall:  0.999\n",
      "[[499   1]\n",
      " [  0 500]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.033111384690739216\n",
      "Acc:  0.9925\n",
      "F1:  0.9924999831249619\n",
      "Precision:  0.9925044325398928\n",
      "Recall:  0.9924999999999999\n",
      "[[1988   12]\n",
      " [  18 1982]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  2.198098463076044\n",
      "Test Acc:  0.511\n",
      "Test F1:  0.35732225491832476\n",
      "Test Precision:  0.7527805864509606\n",
      "Test Recall:  0.511\n",
      "[[ 11 489]\n",
      " [  0 500]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.0335353270447813\n",
      "Acc:  0.99125\n",
      "F1:  0.9912499338276246\n",
      "Precision:  0.991264860762038\n",
      "Recall:  0.99125\n",
      "[[1977   23]\n",
      " [  12 1988]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01931144777659938\n",
      "Test Acc:  0.996\n",
      "Test F1:  0.9959999839999361\n",
      "Test Precision:  0.996007936126978\n",
      "Test Recall:  0.996\n",
      "[[497   3]\n",
      " [  1 499]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.038604012907482686\n",
      "Acc:  0.989\n",
      "F1:  0.988999955999824\n",
      "Precision:  0.9890078241251861\n",
      "Recall:  0.989\n",
      "[[1974   26]\n",
      " [  18 1982]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.5462758621976828\n",
      "Test Acc:  0.763\n",
      "Test F1:  0.7491466222116376\n",
      "Test Precision:  0.837568989860095\n",
      "Test Recall:  0.763\n",
      "[[264 236]\n",
      " [  1 499]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026520064138807357\n",
      "Acc:  0.9935\n",
      "F1:  0.9934999983749996\n",
      "Precision:  0.9935004935004935\n",
      "Recall:  0.9935\n",
      "[[1986   14]\n",
      " [  12 1988]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.01842162562521797\n",
      "Test Acc:  0.996\n",
      "Test F1:  0.9959999839999361\n",
      "Test Precision:  0.996007936126978\n",
      "Test Recall:  0.996\n",
      "[[499   1]\n",
      " [  3 497]]\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.017553280576365068\n",
      "Acc:  0.996\n",
      "F1:  0.9959999989999997\n",
      "Precision:  0.996000496000496\n",
      "Recall:  0.996\n",
      "[[1991    9]\n",
      " [   7 1993]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.17299822252462035\n",
      "Test Acc:  0.941\n",
      "Test F1:  0.9407939035783562\n",
      "Test Precision:  0.947227191413238\n",
      "Test Recall:  0.9410000000000001\n",
      "[[441  59]\n",
      " [  0 500]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.028190730961505324\n",
      "Acc:  0.992\n",
      "F1:  0.9919999819999596\n",
      "Precision:  0.9920044280398523\n",
      "Recall:  0.992\n",
      "[[1981   19]\n",
      " [  13 1987]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.019293354256425838\n",
      "Test Acc:  0.998\n",
      "Test F1:  0.997999991999968\n",
      "Test Precision:  0.99800796812749\n",
      "Test Recall:  0.998\n",
      "[[500   0]\n",
      " [  2 498]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03230687631573528\n",
      "Acc:  0.99325\n",
      "F1:  0.9932499894531086\n",
      "Precision:  0.9932530828317676\n",
      "Recall:  0.99325\n",
      "[[1984   16]\n",
      " [  11 1989]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.014509599333094399\n",
      "Test Acc:  0.995\n",
      "Test F1:  0.9949998749968749\n",
      "Test Precision:  0.995049504950495\n",
      "Test Recall:  0.995\n",
      "[[495   5]\n",
      " [  0 500]]\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.037580854304134846\n",
      "Acc:  0.989\n",
      "F1:  0.988999955999824\n",
      "Precision:  0.9890078241251861\n",
      "Recall:  0.989\n",
      "[[1974   26]\n",
      " [  18 1982]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.02892658971871504\n",
      "Test Acc:  0.992\n",
      "Test F1:  0.9919997119896315\n",
      "Test Precision:  0.9920708582035813\n",
      "Test Recall:  0.992\n",
      "[[493   7]\n",
      " [  1 499]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.020904439315665515\n",
      "Acc:  0.9945\n",
      "F1:  0.9944999944999946\n",
      "Precision:  0.9945019780079121\n",
      "Recall:  0.9945\n",
      "[[1987   13]\n",
      " [   9 1991]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.021068489351622702\n",
      "Test Acc:  0.998\n",
      "Test F1:  0.997999991999968\n",
      "Test Precision:  0.99800796812749\n",
      "Test Recall:  0.998\n",
      "[[498   2]\n",
      " [  0 500]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03196206534933299\n",
      "Acc:  0.99275\n",
      "F1:  0.9927499777968071\n",
      "Precision:  0.9927560362614443\n",
      "Recall:  0.99275\n",
      "[[1982   18]\n",
      " [  11 1989]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.06817285944908621\n",
      "Test Acc:  0.989\n",
      "Test F1:  0.9889986688389294\n",
      "Test Precision:  0.9892367906066536\n",
      "Test Recall:  0.989\n",
      "[[500   0]\n",
      " [ 11 489]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.026413348542526364\n",
      "Acc:  0.99475\n",
      "F1:  0.9947499917968623\n",
      "Precision:  0.9947530922068263\n",
      "Recall:  0.99475\n",
      "[[1987   13]\n",
      " [   8 1992]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.7366235407253117\n",
      "Test Acc:  0.584\n",
      "Test F1:  0.49694296107112457\n",
      "Test Precision:  0.7729257641921398\n",
      "Test Recall:  0.584\n",
      "[[ 84 416]\n",
      " [  0 500]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.018546401182189583\n",
      "Acc:  0.99375\n",
      "F1:  0.9937499902343597\n",
      "Precision:  0.9937530859567872\n",
      "Recall:  0.99375\n",
      "[[1985   15]\n",
      " [  10 1990]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  4.4878246044527925\n",
      "Test Acc:  0.5\n",
      "Test F1:  0.3333333333333333\n",
      "Test Precision:  0.25\n",
      "Test Recall:  0.5\n",
      "[[  0 500]\n",
      " [  0 500]]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

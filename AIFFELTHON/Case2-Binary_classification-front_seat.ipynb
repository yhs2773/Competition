{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d63f582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 10 05:37:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.203.03   Driver Version: 450.203.03   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P0    28W /  70W |   1284MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811c073",
   "metadata": {},
   "source": [
    "# 라이브러리 및 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ac0be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/project/AIFFELTHON'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights/bc_weights')\n",
    "DATA_PATH = os.path.join('data/bc')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'front_seat_train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'front_seat_test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # connected to GPU if 'cuda' is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d2e50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking imgs in a folder\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # prints file names\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # prints img info\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        if i==4:\n",
    "            break # print 4 per folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32aa794",
   "metadata": {},
   "source": [
    "# Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a4eb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize imgs, resize to 224x224\n",
    "# Create pipeline\n",
    "# PyTorch offers various augmentation techniques in torchvision.transforms.Compose\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23f0c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'11_inner_front_seat_train_2000': 0, '14_inner_sheet_dirt_train_2000': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating train dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ba47e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bc/front_seat_train : 0\n",
      "data/bc/front_seat_train/14_inner_sheet_dirt_train_2000 : 2000\n",
      "data/bc/front_seat_train/11_inner_front_seat_train_2000 : 2000\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "819dbc6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "test:  {'11_inner_front_seat_test_500': 0, '14_inner_sheet_dirt_test_500': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating test dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "target_class_num = len(os.listdir(os.path.join(TEST_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f1ef2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bc/front_seat_test : 0\n",
      "data/bc/front_seat_test/14_inner_sheet_dirt_test_500 : 500\n",
      "data/bc/front_seat_test/11_inner_front_seat_test_500 : 500\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c994be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics from sklearn.metrics\n",
    "\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab323f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a8c815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a35afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save best params based on acc\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights/bc_weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'bc_classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd96c589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b32c20f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained resnet50\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe56f2bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.3194926611133038\n",
      "Acc:  0.89575\n",
      "F1:  0.8957390460835292\n",
      "Precision:  0.8959163838603172\n",
      "Recall:  0.89575\n",
      "[[1812  188]\n",
      " [ 229 1771]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.3222171898609966\n",
      "Test Acc:  0.862\n",
      "Test F1:  0.8595528488381561\n",
      "Test Precision:  0.8891201155751238\n",
      "Test Recall:  0.862\n",
      "[[365 135]\n",
      " [  3 497]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.28736963655267445\n",
      "Acc:  0.9065\n",
      "F1:  0.9064960494580896\n",
      "Precision:  0.906568710112009\n",
      "Recall:  0.9065000000000001\n",
      "[[1826  174]\n",
      " [ 200 1800]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.21441101160051768\n",
      "Test Acc:  0.963\n",
      "Test F1:  0.9629866381763816\n",
      "Test Precision:  0.9636695388140475\n",
      "Test Recall:  0.963\n",
      "[[491   9]\n",
      " [ 28 472]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1796588839164802\n",
      "Acc:  0.938\n",
      "F1:  0.9379944039949606\n",
      "Precision:  0.9381581751012116\n",
      "Recall:  0.938\n",
      "[[1895  105]\n",
      " [ 143 1857]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.16344112054631113\n",
      "Test Acc:  0.964\n",
      "Test F1:  0.9639987039533423\n",
      "Test Precision:  0.9640668256228897\n",
      "Test Recall:  0.964\n",
      "[[479  21]\n",
      " [ 15 485]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.19651285941816038\n",
      "Acc:  0.93525\n",
      "F1:  0.9352438441179314\n",
      "Precision:  0.9354155667692641\n",
      "Recall:  0.9352499999999999\n",
      "[[1890  110]\n",
      " [ 149 1851]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.20528174737628432\n",
      "Test Acc:  0.939\n",
      "Test F1:  0.9388531865007883\n",
      "Test Precision:  0.9432570406180962\n",
      "Test Recall:  0.9390000000000001\n",
      "[[445  55]\n",
      " [  6 494]]\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.19130975812200515\n",
      "Acc:  0.9335\n",
      "F1:  0.9334878781657957\n",
      "Precision:  0.9338162520477428\n",
      "Recall:  0.9335\n",
      "[[1894  106]\n",
      " [ 160 1840]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08922110127960332\n",
      "Test Acc:  0.975\n",
      "Test F1:  0.9749993749843746\n",
      "Test Precision:  0.975047504750475\n",
      "Test Recall:  0.975\n",
      "[[485  15]\n",
      " [ 10 490]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1674813831609393\n",
      "Acc:  0.94325\n",
      "F1:  0.9432428166689846\n",
      "Precision:  0.9434745089701662\n",
      "Recall:  0.94325\n",
      "[[1909   91]\n",
      " [ 136 1864]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.1314559441684105\n",
      "Test Acc:  0.957\n",
      "Test F1:  0.9569810286336273\n",
      "Test Precision:  0.9578075725579922\n",
      "Test Recall:  0.9570000000000001\n",
      "[[468  32]\n",
      " [ 11 489]]\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.15538968160630218\n",
      "Acc:  0.9445\n",
      "F1:  0.9444932836873262\n",
      "Precision:  0.9447152421772138\n",
      "Recall:  0.9445\n",
      "[[1911   89]\n",
      " [ 133 1867]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.5689461893346597\n",
      "Test Acc:  0.715\n",
      "Test F1:  0.6901874203047904\n",
      "Test Precision:  0.816342084973898\n",
      "Test Recall:  0.715\n",
      "[[216 284]\n",
      " [  1 499]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.13379176457722983\n",
      "Acc:  0.9545\n",
      "F1:  0.9544944938337538\n",
      "Precision:  0.9547200845209081\n",
      "Recall:  0.9545\n",
      "[[1931   69]\n",
      " [ 113 1887]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.4291327989138663\n",
      "Test Acc:  0.766\n",
      "Test F1:  0.7526887175324675\n",
      "Test Precision:  0.8389813228937282\n",
      "Test Recall:  0.766\n",
      "[[499   1]\n",
      " [233 267]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1352970459099327\n",
      "Acc:  0.951\n",
      "F1:  0.9509964594941984\n",
      "Precision:  0.9511303766788601\n",
      "Recall:  0.9510000000000001\n",
      "[[1919   81]\n",
      " [ 115 1885]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.36587631281465294\n",
      "Test Acc:  0.886\n",
      "Test F1:  0.8858173076923077\n",
      "Test Precision:  0.8884863123993558\n",
      "Test Recall:  0.886\n",
      "[[463  37]\n",
      " [ 77 423]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.09306467160405148\n",
      "Acc:  0.97175\n",
      "F1:  0.9717480771034979\n",
      "Precision:  0.9718784689131617\n",
      "Recall:  0.97175\n",
      "[[1960   40]\n",
      " [  73 1927]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.29673746302630755\n",
      "Test Acc:  0.857\n",
      "Test F1:  0.8540993491606632\n",
      "Test Precision:  0.8878428117626098\n",
      "Test Recall:  0.857\n",
      "[[358 142]\n",
      " [  1 499]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.06330709413640083\n",
      "Acc:  0.98075\n",
      "F1:  0.9807493635258315\n",
      "Precision:  0.9808135875969597\n",
      "Recall:  0.98075\n",
      "[[1973   27]\n",
      " [  50 1950]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2339823303818034\n",
      "Test Acc:  0.924\n",
      "Test F1:  0.9235815324718157\n",
      "Test Precision:  0.9334952806268505\n",
      "Test Recall:  0.9239999999999999\n",
      "[[499   1]\n",
      " [ 75 425]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03396581666958001\n",
      "Acc:  0.9895\n",
      "F1:  0.9894999973749994\n",
      "Precision:  0.9895004895004895\n",
      "Recall:  0.9895\n",
      "[[1980   20]\n",
      " [  22 1978]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.036471669596545596\n",
      "Test Acc:  0.981\n",
      "Test F1:  0.9809945074126423\n",
      "Test Precision:  0.9815566795215269\n",
      "Test Recall:  0.981\n",
      "[[499   1]\n",
      " [ 18 482]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.04418643563616252\n",
      "Acc:  0.987\n",
      "F1:  0.9869999187494922\n",
      "Precision:  0.9870121753043826\n",
      "Recall:  0.9870000000000001\n",
      "[[1979   21]\n",
      " [  31 1969]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.1767508911402565\n",
      "Test Acc:  0.932\n",
      "Test F1:  0.9316841073122117\n",
      "Test Precision:  0.9401408450704225\n",
      "Test Recall:  0.9319999999999999\n",
      "[[500   0]\n",
      " [ 68 432]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.036686691475869525\n",
      "Acc:  0.989\n",
      "F1:  0.9889999312495703\n",
      "Precision:  0.9890122253056326\n",
      "Recall:  0.9890000000000001\n",
      "[[1983   17]\n",
      " [  27 1973]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2861796256213092\n",
      "Test Acc:  0.873\n",
      "Test F1:  0.870984126984127\n",
      "Test Precision:  0.8978666666666666\n",
      "Test Recall:  0.873\n",
      "[[374 126]\n",
      " [  1 499]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03847413336754673\n",
      "Acc:  0.9885\n",
      "F1:  0.9885\n",
      "Precision:  0.9885\n",
      "Recall:  0.9885\n",
      "[[1977   23]\n",
      " [  23 1977]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.027076250303711278\n",
      "Test Acc:  0.997\n",
      "Test F1:  0.996999972999757\n",
      "Test Precision:  0.9970178926441352\n",
      "Test Recall:  0.997\n",
      "[[497   3]\n",
      " [  0 500]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.041007176629223284\n",
      "Acc:  0.98525\n",
      "F1:  0.9852499253277469\n",
      "Precision:  0.9852598265114869\n",
      "Recall:  0.98525\n",
      "[[1975   25]\n",
      " [  34 1966]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.02085243211212273\n",
      "Test Acc:  0.995\n",
      "Test F1:  0.9949998749968749\n",
      "Test Precision:  0.995049504950495\n",
      "Test Recall:  0.995\n",
      "[[500   0]\n",
      " [  5 495]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.04467092142329507\n",
      "Acc:  0.9885\n",
      "F1:  0.9884998964990686\n",
      "Precision:  0.9885175866331188\n",
      "Recall:  0.9885\n",
      "[[1983   17]\n",
      " [  29 1971]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.03554455773733571\n",
      "Test Acc:  0.989\n",
      "Test F1:  0.9889986688389294\n",
      "Test Precision:  0.9892367906066536\n",
      "Test Recall:  0.989\n",
      "[[489  11]\n",
      " [  0 500]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03709922897599874\n",
      "Acc:  0.98975\n",
      "F1:  0.9897499942343717\n",
      "Precision:  0.9897511019399794\n",
      "Recall:  0.98975\n",
      "[[1981   19]\n",
      " [  22 1978]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.07674120285984827\n",
      "Test Acc:  0.966\n",
      "Test F1:  0.9659606505119919\n",
      "Test Precision:  0.9681647940074907\n",
      "Test Recall:  0.966\n",
      "[[466  34]\n",
      " [  0 500]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.025480828469707853\n",
      "Acc:  0.99425\n",
      "F1:  0.994249999640625\n",
      "Precision:  0.9942501235625308\n",
      "Recall:  0.9942500000000001\n",
      "[[1989   11]\n",
      " [  12 1988]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.6703187853191994\n",
      "Test Acc:  0.506\n",
      "Test F1:  0.3465297289288908\n",
      "Test Precision:  0.7515090543259557\n",
      "Test Recall:  0.506\n",
      "[[  6 494]\n",
      " [  0 500]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02537812780781043\n",
      "Acc:  0.9915\n",
      "F1:  0.991499965999864\n",
      "Precision:  0.991507864125826\n",
      "Recall:  0.9915\n",
      "[[1987   13]\n",
      " [  21 1979]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08795220134486678\n",
      "Test Acc:  0.967\n",
      "Test F1:  0.9669640238219421\n",
      "Test Precision:  0.9690431519699813\n",
      "Test Recall:  0.9670000000000001\n",
      "[[467  33]\n",
      " [  0 500]]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e5dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/0_felton'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data_ex_rear')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # connected to GPU if 'cuda' is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cbea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking imgs in a folder\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # prints file names\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # prints img info\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        break # print 1 per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb641cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize imgs, resize to 224x224\n",
    "# Create pipeline\n",
    "# PyTorch offers various augmentation techniques in torchvision.transforms.Compose\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db993bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'12_inner_rear_seat_train_2000': 0, 'resized_14_inner_sheet_dirt_train': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating train dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4deaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_rear/train : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_rear/train/12_inner_rear_seat_train_2000 : 2000\n",
      "/aiffel/aiffel/0_felton/data_ex_rear/train/resized_14_inner_sheet_dirt_train : 1036\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2cbd6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  {'12_inner_rear_seat_test_500': 0, 'resized_14_inner_sheet_dirt_test': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating test dataset\n",
    "# shuffle = False\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03b0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/0_felton/data_ex_rear/test : 0\n",
      "/aiffel/aiffel/0_felton/data_ex_rear/test/resized_14_inner_sheet_dirt_test : 592\n",
      "/aiffel/aiffel/0_felton/data_ex_rear/test/12_inner_rear_seat_test_500 : 500\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847fc38",
   "metadata": {},
   "source": [
    "### model 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef09a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics from sklearn.metrics\n",
    "\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b7efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd64342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9988de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save best params based on acc\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e3954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /aiffel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784d4e1987474effabed66628a1a736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained resnet50\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835943be",
   "metadata": {},
   "source": [
    "### 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c868ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.6718214518145511\n",
      "Acc:  0.6577733860342556\n",
      "F1:  0.5668464376171916\n",
      "Precision:  0.5977589961861732\n",
      "Recall:  0.5697355212355213\n",
      "[[1694  306]\n",
      " [ 733  303]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.6971654175019963\n",
      "Test Acc:  0.5421245421245421\n",
      "Test F1:  0.3515439429928741\n",
      "Test Precision:  0.27106227106227104\n",
      "Test Recall:  0.5\n",
      "[[  0 500]\n",
      " [  0 592]]\n",
      "[Notification] Best Model Updated!\n"
     ]
    }
   ],
   "source": [
    "# test with 1 epoch\n",
    " \n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53067ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.44231411344126653\n",
      "Acc:  0.8040184453227931\n",
      "F1:  0.7788058316540485\n",
      "Precision:  0.7835039002724592\n",
      "Recall:  0.7749488416988417\n",
      "[[1733  267]\n",
      " [ 328  708]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.20299427407258924\n",
      "Test Acc:  0.9386446886446886\n",
      "Test F1:  0.9381767881572266\n",
      "Test Precision:  0.9383950763403953\n",
      "Test Recall:  0.937972972972973\n",
      "[[465  35]\n",
      " [ 32 560]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.2583443295406668\n",
      "Acc:  0.8982213438735178\n",
      "F1:  0.8856959883498424\n",
      "Precision:  0.8897921749856583\n",
      "Recall:  0.8820405405405405\n",
      "[[1866  134]\n",
      " [ 175  861]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.34083774235946995\n",
      "Test Acc:  0.8690476190476191\n",
      "Test F1:  0.8689895002663737\n",
      "Test Precision:  0.8804468988086107\n",
      "Test Recall:  0.8770472972972974\n",
      "[[486  14]\n",
      " [129 463]]\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.19597093976641955\n",
      "Acc:  0.9278656126482213\n",
      "F1:  0.9190675441356639\n",
      "Precision:  0.9230357013862169\n",
      "Recall:  0.9154739382239383\n",
      "[[1909   91]\n",
      " [ 128  908]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.30273723658560664\n",
      "Test Acc:  0.8736263736263736\n",
      "Test F1:  0.8677599502967801\n",
      "Test Precision:  0.9054794520547945\n",
      "Test Recall:  0.862\n",
      "[[362 138]\n",
      " [  0 592]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1680940611974189\n",
      "Acc:  0.9377470355731226\n",
      "F1:  0.9308790383170549\n",
      "Precision:  0.9301902295618558\n",
      "Recall:  0.9315810810810811\n",
      "[[1902   98]\n",
      " [  91  945]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.3018345870129315\n",
      "Test Acc:  0.8818681318681318\n",
      "Test F1:  0.8792431997791765\n",
      "Test Precision:  0.8903106802250685\n",
      "Test Recall:  0.8756621621621622\n",
      "[[401  99]\n",
      " [ 30 562]]\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.17829755270167402\n",
      "Acc:  0.9351119894598156\n",
      "F1:  0.9280195284993404\n",
      "Precision:  0.9269557062410594\n",
      "Recall:  0.9291158301158301\n",
      "[[1896  104]\n",
      " [  93  943]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.1599082402351868\n",
      "Test Acc:  0.9404761904761905\n",
      "Test F1:  0.9393776342928885\n",
      "Test Precision:  0.9480141708490353\n",
      "Test Recall:  0.935777027027027\n",
      "[[440  60]\n",
      " [  5 587]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.15440166084782073\n",
      "Acc:  0.9397233201581028\n",
      "F1:  0.9331348919562402\n",
      "Precision:  0.9320582689030354\n",
      "Recall:  0.9342442084942085\n",
      "[[1903   97]\n",
      " [  86  950]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.1440806763347986\n",
      "Test Acc:  0.9761904761904762\n",
      "Test F1:  0.97606200002698\n",
      "Test Precision:  0.9754108297413793\n",
      "Test Recall:  0.9769527027027027\n",
      "[[493   7]\n",
      " [ 19 573]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1362736857643253\n",
      "Acc:  0.9525691699604744\n",
      "F1:  0.9473727546532036\n",
      "Precision:  0.9463589457805279\n",
      "Recall:  0.9484140926640927\n",
      "[[1923   77]\n",
      " [  67  969]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.3961274692891961\n",
      "Test Acc:  0.8351648351648352\n",
      "Test F1:  0.8252588757396448\n",
      "Test Precision:  0.8764901712111604\n",
      "Test Recall:  0.8209324324324325\n",
      "[[326 174]\n",
      " [  6 586]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1475875312756551\n",
      "Acc:  0.950592885375494\n",
      "F1:  0.945205097057006\n",
      "Precision:  0.9440008370835702\n",
      "Recall:  0.9464488416988417\n",
      "[[1919   81]\n",
      " [  69  967]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.7420240276685666\n",
      "Test Acc:  0.5787545787545788\n",
      "Test F1:  0.43417139767504737\n",
      "Test Precision:  0.7813688212927756\n",
      "Test Recall:  0.54\n",
      "[[ 40 460]\n",
      " [  0 592]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.15081861121089835\n",
      "Acc:  0.9403820816864296\n",
      "F1:  0.9339261569649964\n",
      "Precision:  0.9324709344824529\n",
      "Recall:  0.9354420849420849\n",
      "[[1902   98]\n",
      " [  83  953]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.11298977876596017\n",
      "Test Acc:  0.9743589743589743\n",
      "Test F1:  0.9741507870753935\n",
      "Test Precision:  0.9746523499383911\n",
      "Test Recall:  0.9737094594594595\n",
      "[[483  17]\n",
      " [ 11 581]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.13905093585582157\n",
      "Acc:  0.9482872200263505\n",
      "F1:  0.9426611306891282\n",
      "Precision:  0.9413674585472875\n",
      "Recall:  0.9440009652509653\n",
      "[[1915   85]\n",
      " [  72  964]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.12705914177414532\n",
      "Test Acc:  0.9633699633699634\n",
      "Test F1:  0.9630601861874866\n",
      "Test Precision:  0.963739837398374\n",
      "Test Recall:  0.9624864864864865\n",
      "[[476  24]\n",
      " [ 16 576]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.13417637359939125\n",
      "Acc:  0.9499341238471674\n",
      "F1:  0.9446507158302081\n",
      "Precision:  0.9421404816284009\n",
      "Recall:  0.9473445945945946\n",
      "[[1911   89]\n",
      " [  63  973]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2854683547444979\n",
      "Test Acc:  0.8882783882783882\n",
      "Test F1:  0.887444365966334\n",
      "Test Precision:  0.8875704839560261\n",
      "Test Recall:  0.8873243243243243\n",
      "[[438  62]\n",
      " [ 60 532]]\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12542379469071563\n",
      "Acc:  0.9535573122529645\n",
      "F1:  0.9486215188407328\n",
      "Precision:  0.9463626320246574\n",
      "Recall:  0.9510250965250965\n",
      "[[1918   82]\n",
      " [  59  977]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2453955902459649\n",
      "Test Acc:  0.9285714285714286\n",
      "Test F1:  0.92855609088347\n",
      "Test Precision:  0.9321907299741602\n",
      "Test Recall:  0.9339662162162162\n",
      "[[499   1]\n",
      " [ 77 515]]\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12661014109065657\n",
      "Acc:  0.953227931488801\n",
      "F1:  0.9480077550829417\n",
      "Precision:  0.947800401557627\n",
      "Recall:  0.9482162162162162\n",
      "[[1928   72]\n",
      " [  70  966]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.09395981591680519\n",
      "Test Acc:  0.9661172161172161\n",
      "Test F1:  0.9657634710369845\n",
      "Test Precision:  0.9677312336886805\n",
      "Test Recall:  0.9643986486486487\n",
      "[[472  28]\n",
      " [  9 583]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12738943519560914\n",
      "Acc:  0.9519104084321476\n",
      "F1:  0.9469070050901509\n",
      "Precision:  0.9438516260162602\n",
      "Recall:  0.9502403474903476\n",
      "[[1911   89]\n",
      " [  57  979]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.5477302935269308\n",
      "Test Acc:  0.73992673992674\n",
      "Test F1:  0.7391909847784038\n",
      "Test Precision:  0.7572850678733032\n",
      "Test Recall:  0.749722972972973\n",
      "[[433  67]\n",
      " [217 375]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1494732833418407\n",
      "Acc:  0.946969696969697\n",
      "F1:  0.9413072476656292\n",
      "Precision:  0.9392679425356977\n",
      "Recall:  0.9434662162162162\n",
      "[[1909   91]\n",
      " [  70  966]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.14109715900216568\n",
      "Test Acc:  0.967948717948718\n",
      "Test F1:  0.967812649746552\n",
      "Test Precision:  0.9669990016504701\n",
      "Test Recall:  0.9693513513513514\n",
      "[[493   7]\n",
      " [ 28 564]]\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12609317984039847\n",
      "Acc:  0.9538866930171278\n",
      "F1:  0.9489048146609427\n",
      "Precision:  0.9473027566539924\n",
      "Recall:  0.95057722007722\n",
      "[[1922   78]\n",
      " [  62  974]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.13773425259445263\n",
      "Test Acc:  0.9606227106227107\n",
      "Test F1:  0.9604048560464051\n",
      "Test Precision:  0.9598118501402871\n",
      "Test Recall:  0.9611959459459459\n",
      "[[484  16]\n",
      " [ 27 565]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12220821878627727\n",
      "Acc:  0.955862977602108\n",
      "F1:  0.951116868011314\n",
      "Precision:  0.9493151839037102\n",
      "Recall:  0.953007722007722\n",
      "[[1924   76]\n",
      " [  58  978]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.6431411018865288\n",
      "Test Acc:  0.7316849816849816\n",
      "Test F1:  0.6935988791837884\n",
      "Test Precision:  0.8344632768361582\n",
      "Test Recall:  0.707\n",
      "[[207 293]\n",
      " [  0 592]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.120924084192436\n",
      "Acc:  0.9561923583662714\n",
      "F1:  0.9515146828542154\n",
      "Precision:  0.9494275990616707\n",
      "Recall:  0.953722972972973\n",
      "[[1923   77]\n",
      " [  56  980]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.2581038527482275\n",
      "Test Acc:  0.9038461538461539\n",
      "Test F1:  0.9033141833691288\n",
      "Test Precision:  0.9028010279867023\n",
      "Test Recall:  0.9040135135135134\n",
      "[[453  47]\n",
      " [ 58 534]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.1254020622588302\n",
      "Acc:  0.9509222661396575\n",
      "F1:  0.9456818627464518\n",
      "Precision:  0.943622081046829\n",
      "Recall:  0.9478619691119692\n",
      "[[1915   85]\n",
      " [  64  972]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.0846912026388655\n",
      "Test Acc:  0.7408424908424909\n",
      "Test F1:  0.7078433504289665\n",
      "Test Precision:  0.8302114571318724\n",
      "Test Recall:  0.7176216216216216\n",
      "[[221 279]\n",
      " [  4 588]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12769013036434587\n",
      "Acc:  0.9555335968379447\n",
      "F1:  0.950695876707212\n",
      "Precision:  0.94937853915081\n",
      "Recall:  0.9520598455598455\n",
      "[[1926   74]\n",
      " [  61  975]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08159970150064266\n",
      "Test Acc:  0.9688644688644689\n",
      "Test F1:  0.9685334598955861\n",
      "Test Precision:  0.9706495098039216\n",
      "Test Recall:  0.9670878378378378\n",
      "[[473  27]\n",
      " [  7 585]]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

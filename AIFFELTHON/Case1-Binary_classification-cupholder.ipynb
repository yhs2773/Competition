{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba078a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  4 02:49:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.203.03   Driver Version: 450.203.03   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7e913",
   "metadata": {},
   "source": [
    "# 라이브러리 및 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b64e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/project/AIFFELTHON'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights/bc_weights')\n",
    "DATA_PATH = os.path.join('data')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # connected to GPU if 'cuda' is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991f1e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking imgs in a folder\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        print(os.path.join(dirpath, filename)) # prints file names\n",
    "        image = Image.open(os.path.join(dirpath, filename), 'r')\n",
    "        print(f'size: ({image.width}, {image.height}, {image.getbands()})') # prints img info\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        if i==4:\n",
    "            break # print 4 per folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3eb493",
   "metadata": {},
   "source": [
    "# Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5939329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize imgs, resize to 224x224\n",
    "# Create pipeline\n",
    "# PyTorch offers various augmentation techniques in torchvision.transforms.Compose\n",
    "\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe4feea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "train:  {'07_inner_cupholder_refined2_train': 0, '08_inner_cupholder_dirt_refined2_train': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating train dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b52032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train : 0\n",
      "data/train/07_inner_cupholder_refined2_train : 1803\n",
      "data/train/08_inner_cupholder_dirt_refined2_train : 1926\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cfc6eb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_class_num:  2\n",
      "test:  {'07_inner_cupholder_refined2_test': 0, '08_inner_cupholder_dirt_refined2_test': 1}\n"
     ]
    }
   ],
   "source": [
    "# creating test dataset\n",
    "\n",
    "BATCH_SIZE = 64 # changed from 64 to 1\n",
    "\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "target_class_num = len(os.listdir(os.path.join(TEST_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a5f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test : 0\n",
      "data/test/08_inner_cupholder_dirt_refined2_test : 100\n",
      "data/test/07_inner_cupholder_refined2_test : 100\n"
     ]
    }
   ],
   "source": [
    "# checking num of imgs in each class\n",
    "\n",
    "for rootpath, dirpath, filenames in os.walk(TEST_PATH):\n",
    "    print(f'{rootpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4bdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics from sklearn.metrics\n",
    "\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deff636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss: ', mean(train_losses))\n",
    "    print('Acc: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Precision: ', prec)\n",
    "    print('Recall: ', rec)\n",
    "    print(confusion_matrix(train_trues, train_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6de3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Test Loss: ', mean(test_losses))\n",
    "    print('Test Acc: ', acc)\n",
    "    print('Test F1: ', f1)\n",
    "    print('Test Precision: ', prec)\n",
    "    print('Test Recall: ', rec)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a6b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save best params based on acc\n",
    "\n",
    "def train_classifier(net, train_loader, test_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights/bc_weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(test_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'bc_classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6c0497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4070e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /aiffel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6437238083e64f3196278897612ba062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained resnet50\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a967eac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Start Training Model!\n",
      "> epoch:  0\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.589007462990486\n",
      "Acc:  0.7489943684633951\n",
      "F1:  0.7403867781381095\n",
      "Precision:  0.7735351305663811\n",
      "Recall:  0.74379754752809\n",
      "[[1057  746]\n",
      " [ 190 1736]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.061807417700926\n",
      "Test Acc:  0.68\n",
      "Test F1:  0.6483516483516484\n",
      "Test Precision:  0.78125\n",
      "Test Recall:  0.6799999999999999\n",
      "[[38 62]\n",
      " [ 2 98]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  1\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.6260151504460028\n",
      "Acc:  0.7299544113703406\n",
      "F1:  0.725443093810999\n",
      "Precision:  0.738154306170842\n",
      "Recall:  0.7265167261901677\n",
      "[[1122  681]\n",
      " [ 326 1600]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.5725869910046457\n",
      "Test Acc:  0.5\n",
      "Test F1:  0.3333333333333333\n",
      "Test Precision:  0.25\n",
      "Test Recall:  0.5\n",
      "[[  0 100]\n",
      " [  0 100]]\n",
      "> epoch:  2\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.12491080115975464\n",
      "Acc:  0.9562885492089032\n",
      "F1:  0.956213871114721\n",
      "Precision:  0.9565941443057335\n",
      "Recall:  0.9559664318555263\n",
      "[[1706   97]\n",
      " [  66 1860]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.799204286937602\n",
      "Test Acc:  0.5\n",
      "Test F1:  0.3333333333333333\n",
      "Test Precision:  0.25\n",
      "Test Recall:  0.5\n",
      "[[  0 100]\n",
      " [  0 100]]\n",
      "> epoch:  3\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.0462723713194541\n",
      "Acc:  0.9857870742826496\n",
      "F1:  0.9857681441094497\n",
      "Precision:  0.9859038668207134\n",
      "Recall:  0.9856564776946695\n",
      "[[1770   33]\n",
      " [  20 1906]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.013814613804627172\n",
      "Test Acc:  0.995\n",
      "Test F1:  0.9949998749968749\n",
      "Test Precision:  0.995049504950495\n",
      "Test Recall:  0.995\n",
      "[[100   0]\n",
      " [  1  99]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  4\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.05493754974242805\n",
      "Acc:  0.9836417270045589\n",
      "F1:  0.9836258821183583\n",
      "Precision:  0.9835728207889123\n",
      "Recall:  0.9836858956083924\n",
      "[[1776   27]\n",
      " [  34 1892]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  1.4522444563449244\n",
      "Test Acc:  0.595\n",
      "Test F1:  0.515535751667215\n",
      "Test Precision:  0.7762430939226519\n",
      "Test Recall:  0.595\n",
      "[[ 19  81]\n",
      " [  0 100]]\n",
      "> epoch:  5\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.0266517497977968\n",
      "Acc:  0.9914186108876374\n",
      "F1:  0.9914073490323763\n",
      "Precision:  0.9915323089475585\n",
      "Recall:  0.9913030031290874\n",
      "[[1781   22]\n",
      " [  10 1916]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08115408690995438\n",
      "Test Acc:  0.985\n",
      "Test F1:  0.9849966242404541\n",
      "Test Precision:  0.9854368932038835\n",
      "Test Recall:  0.985\n",
      "[[100   0]\n",
      " [  3  97]]\n",
      "> epoch:  6\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02463421226337047\n",
      "Acc:  0.9927594529364441\n",
      "F1:  0.9927506424180912\n",
      "Precision:  0.99281872575373\n",
      "Recall:  0.9926895810547668\n",
      "[[1786   17]\n",
      " [  10 1916]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.22271092632605075\n",
      "Test Acc:  0.91\n",
      "Test F1:  0.9094202898550725\n",
      "Test Precision:  0.9207717569786535\n",
      "Test Recall:  0.9099999999999999\n",
      "[[83 17]\n",
      " [ 1 99]]\n",
      "> epoch:  7\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02392656748356708\n",
      "Acc:  0.9935639581657281\n",
      "F1:  0.9935564842952624\n",
      "Precision:  0.9935937351493489\n",
      "Recall:  0.9935215278101744\n",
      "[[1789   14]\n",
      " [  10 1916]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  2.2050104832206854\n",
      "Test Acc:  0.5\n",
      "Test F1:  0.3333333333333333\n",
      "Test Precision:  0.25\n",
      "Test Recall:  0.5\n",
      "[[  0 100]\n",
      " [  0 100]]\n",
      "> epoch:  8\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.017912915281327914\n",
      "Acc:  0.9932957897559668\n",
      "F1:  0.993287631868603\n",
      "Precision:  0.993355787477374\n",
      "Recall:  0.9932265020396951\n",
      "[[1787   16]\n",
      " [   9 1917]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.8649282405359554\n",
      "Test Acc:  0.55\n",
      "Test F1:  0.4357366771159875\n",
      "Test Precision:  0.763157894736842\n",
      "Test Recall:  0.55\n",
      "[[ 10  90]\n",
      " [  0 100]]\n",
      "> epoch:  9\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.03654043592800686\n",
      "Acc:  0.9924912845266828\n",
      "F1:  0.9924848634907109\n",
      "Precision:  0.9923906998754211\n",
      "Recall:  0.9926070775084102\n",
      "[[1796    7]\n",
      " [  21 1905]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  2.1363124467579473\n",
      "Test Acc:  0.5\n",
      "Test F1:  0.3333333333333333\n",
      "Test Precision:  0.25\n",
      "Test Recall:  0.5\n",
      "[[  0 100]\n",
      " [  0 100]]\n",
      "> epoch:  10\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.022413641649694973\n",
      "Acc:  0.9924912845266828\n",
      "F1:  0.9924811358689829\n",
      "Precision:  0.9926312286423908\n",
      "Recall:  0.9923591349136003\n",
      "[[1782   21]\n",
      " [   7 1919]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.008479055303479583\n",
      "Test Acc:  1.0\n",
      "Test F1:  1.0\n",
      "Test Precision:  1.0\n",
      "Test Recall:  1.0\n",
      "[[100   0]\n",
      " [  0 100]]\n",
      "[Notification] Best Model Updated!\n",
      "> epoch:  11\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.015555860949246119\n",
      "Acc:  0.9954411370340573\n",
      "F1:  0.9954360904718589\n",
      "Precision:  0.9954450080548223\n",
      "Recall:  0.9954273165354386\n",
      "[[1794    9]\n",
      " [   8 1918]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.03699042320156878\n",
      "Test Acc:  0.99\n",
      "Test F1:  0.98999899989999\n",
      "Test Precision:  0.9901960784313726\n",
      "Test Recall:  0.99\n",
      "[[ 98   2]\n",
      " [  0 100]]\n",
      "> epoch:  12\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.01156000342808064\n",
      "Acc:  0.99597747385358\n",
      "F1:  0.9959728760403944\n",
      "Precision:  0.9960005184033178\n",
      "Recall:  0.9959465273350232\n",
      "[[1794    9]\n",
      " [   6 1920]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.12182267219919594\n",
      "Test Acc:  0.93\n",
      "Test F1:  0.9296553110240177\n",
      "Test Precision:  0.9385964912280702\n",
      "Test Recall:  0.9299999999999999\n",
      "[[ 86  14]\n",
      " [  0 100]]\n",
      "> epoch:  13\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.02681456241555417\n",
      "Acc:  0.9908822740681148\n",
      "F1:  0.9908741880873411\n",
      "Precision:  0.9907903486273393\n",
      "Recall:  0.990978604368282\n",
      "[[1792   11]\n",
      " [  23 1903]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.23884416731481906\n",
      "Test Acc:  0.91\n",
      "Test F1:  0.9092650468797258\n",
      "Test Precision:  0.923728813559322\n",
      "Test Recall:  0.9099999999999999\n",
      "[[ 82  18]\n",
      " [  0 100]]\n",
      "> epoch:  14\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.011766889069172143\n",
      "Acc:  0.9965138106731027\n",
      "F1:  0.9965098259016751\n",
      "Precision:  0.9965374978399861\n",
      "Recall:  0.9964834483199514\n",
      "[[1795    8]\n",
      " [   5 1921]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.12315721970344867\n",
      "Test Acc:  0.975\n",
      "Test F1:  0.9749943737340901\n",
      "Test Precision:  0.975427885096587\n",
      "Test Recall:  0.975\n",
      "[[99  1]\n",
      " [ 4 96]]\n",
      "> epoch:  15\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.016893170687164796\n",
      "Acc:  0.995172968624296\n",
      "F1:  0.9951671850699844\n",
      "Precision:  0.9952249608560853\n",
      "Recall:  0.9951145805796155\n",
      "[[1791   12]\n",
      " [   6 1920]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08685589523685337\n",
      "Test Acc:  0.975\n",
      "Test F1:  0.9749843652282677\n",
      "Test Precision:  0.9761904761904762\n",
      "Test Recall:  0.975\n",
      "[[ 95   5]\n",
      " [  0 100]]\n",
      "> epoch:  16\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.015564649386560324\n",
      "Acc:  0.995172968624296\n",
      "F1:  0.9951675385764649\n",
      "Precision:  0.9951856506643382\n",
      "Recall:  0.9951500009503027\n",
      "[[1793   10]\n",
      " [   8 1918]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.0393697269819722\n",
      "Test Acc:  0.985\n",
      "Test F1:  0.9849966242404541\n",
      "Test Precision:  0.9854368932038835\n",
      "Test Recall:  0.985\n",
      "[[ 97   3]\n",
      " [  0 100]]\n",
      "> epoch:  17\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.012154523683790813\n",
      "Acc:  0.9962456422633413\n",
      "F1:  0.9962412825055698\n",
      "Precision:  0.9962787328596485\n",
      "Recall:  0.9962061327348155\n",
      "[[1794    9]\n",
      " [   5 1921]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.04143075218662851\n",
      "Test Acc:  0.985\n",
      "Test F1:  0.9849996249906248\n",
      "Test Precision:  0.9850485048504851\n",
      "Test Recall:  0.985\n",
      "[[99  1]\n",
      " [ 2 98]]\n",
      "> epoch:  18\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.012628655539336219\n",
      "Acc:  0.9962456422633413\n",
      "F1:  0.9962411439433212\n",
      "Precision:  0.9962990422270381\n",
      "Recall:  0.9961884225494719\n",
      "[[1793   10]\n",
      " [   4 1922]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.24676135741581676\n",
      "Test Acc:  0.91\n",
      "Test F1:  0.9092650468797258\n",
      "Test Precision:  0.923728813559322\n",
      "Test Recall:  0.9099999999999999\n",
      "[[ 82  18]\n",
      " [  0 100]]\n",
      "> epoch:  19\n",
      "\n",
      "====== Training Metrics ======\n",
      "Loss:  0.017687923959614237\n",
      "Acc:  0.9957093054438187\n",
      "F1:  0.9957041645066529\n",
      "Precision:  0.9957620015415617\n",
      "Recall:  0.9956515015645437\n",
      "[[1792   11]\n",
      " [   5 1921]]\n",
      "====== Test Metrics ======\n",
      "Test Loss:  0.08634743955233716\n",
      "Test Acc:  0.96\n",
      "Test F1:  0.9599358974358974\n",
      "Test Precision:  0.962962962962963\n",
      "Test Recall:  0.96\n",
      "[[ 92   8]\n",
      " [  0 100]]\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, test_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

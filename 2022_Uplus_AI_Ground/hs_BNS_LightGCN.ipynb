{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1I4HMAmBSp1zEOH2jnl1yl5ew4zpE2qhS","authorship_tag":"ABX9TyOhHN/glPaGuwEOZCFFmQcq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"a2L-eLYeOEhv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669960554897,"user_tz":-540,"elapsed":3176,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"9a6737e5-13bf-40d4-b740-b52e1bec1259"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 패키지 로드\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import argparse\n","import datetime\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict\n","import os, random, sys, math, heapq\n","import time\n","\n","from scipy.sparse import csr_matrix\n","import scipy.sparse as sp\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.init import normal_\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.nn.functional as F\n","\n","import plotnine\n","from plotnine import *"],"metadata":{"id":"W69RTIGCsTkf","executionInfo":{"status":"ok","timestamp":1669960562105,"user_tz":-540,"elapsed":7214,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# hp\n","class cfg:\n","  batch_size = 128\n","  epochs = 1\n","  l2 = 1e-4 # l2 penalty\n","  lr = 0.001 # learning rate\n","  lr_dc = 0.1 # lr decay rate\n","  lr_dc_epoch = [20, 40, 60, 80] # epoch which the lr decay\n","  patience = 5\n","  dim = 32 # dim of vector\n","  hop = 1 # num of LGCN layers\n","  num_negatives = 5 # num of neg instances\n","  alpha = 5 # weight\n","  topk = [25] # recommendation list\n","  check_epoch = 1"],"metadata":{"id":"jtPToEw-Sls2","executionInfo":{"status":"ok","timestamp":1669960562106,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Data.py"],"metadata":{"id":"OElUyA8dsshX"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"GTeDjv4OsE08","executionInfo":{"status":"ok","timestamp":1669960562109,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"outputs":[],"source":["class Data(object):\n","    def __init__(self, path, num_user, num_item, status='train'):\n","        self.num_user = num_user\n","        self.num_item = num_item\n","        self.status = status\n","        if self.status == 'train':\n","            self.train_dict, self.prior, self.popularity, self.train_pair = self.load_train_data(path)\n","            self.train_pair = np.asarray(self.train_pair)\n","            self.train_user = [pair[0] for pair in self.train_pair]\n","            self.train_item = [pair[1] for pair in self.train_pair]\n","            self.length = len(self.train_pair)\n","           \n","            self.UserItemNet = csr_matrix((np.ones(len(self.train_user)), (self.train_user, self.train_item)),\n","                                          shape=(self.num_user, self.num_item))\n","            self.Lap_mat, self.Adj_mat = self.build_graph()\n","        else:\n","            self.test_dict, self.test_label = self.load_test_data(path)\n","\n","    def load_train_data(self, path):\n","        data = pd.read_csv(path, header=0) # removed sep=','\n","        data_dict = {}\n","        datapair = []\n","        popularity = np.zeros(self.num_item)\n","\n","        for i in data.itertuples():\n","            # user, item, rating = getattr(i, 'profile_id'), getattr(i, 'album_id'), getattr(i, 'rating') # user > profile_id / item > album_id / rating col added at main\n","            user = i.profile_id\n","            item = i.album_id\n","            rating = i.rating\n","\n","            user, item, rating = int(user), int(item), int(rating)\n","            \n","            popularity[item] += rating\n","            data_dict.setdefault(user, {})\n","            data_dict[user][item] = 1\n","            datapair.append((user, item))\n","        prior = popularity / sum(popularity)\n","        random.shuffle(datapair)\n","        return data_dict, prior, popularity**0.75, datapair\n","\n","    def load_test_data(self, path):\n","        data = pd.read_csv(path, header=0) # removed sep=','\n","        label = np.zeros((self.num_user, self.num_item))\n","        data_dict = {}\n","        for i in data.itertuples():\n","            user, item = getattr(i, 'profile_id'), getattr(i, 'album_id') # user > profile_id / item > album_id\n","            data_dict.setdefault(user, set())\n","            data_dict[user].add(item)\n","            label[user, item] = 1\n","        return data_dict, label\n","\n","    def build_graph(self):\n","        print('building graph adjacency matrix')\n","        st = time.time()\n","        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item),\n","                                dtype=np.float32)\n","        adj_mat = adj_mat.tolil()\n","        R = self.UserItemNet.tolil()\n","        adj_mat[:self.num_user, self.num_user:] = R\n","        adj_mat[self.num_user:, :self.num_user] = R.T\n","        adj_mat = adj_mat.todok()\n","\n","        rowsum = np.array(adj_mat.sum(axis=1))\n","\n","        d_inv = np.power(rowsum, -0.5).flatten()\n","        d_inv[np.isinf(d_inv)] = 0.\n","        d_mat = sp.diags(d_inv)\n","\n","        norm_adj = d_mat.dot(adj_mat)\n","        norm_adj = norm_adj.dot(d_mat)\n","        norm_adj = norm_adj.tocsr()\n","        end = time.time()\n","        print(f\"costing {end - st}s, obtained norm_mat...\")\n","\n","        return norm_adj, adj_mat\n","\n","    def generate_batch(self, batch_size):\n","        n_batch = self.length // batch_size\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = np.arange(self.length - batch_size, self.length)\n","        return slices\n","\n","    def get_slices(self, index):\n","        pairs = self.train_pair[index]\n","        users, items = [], []\n","        for u, i in pairs:\n","            users.append(u)\n","            items.append(i)\n","        return users, items\n","\n","def get_number_of_users_items(file):\n","    data = pd.read_csv(file, header=0, dtype='int') # removed sep=',', changed dtype=str > int\n","    return (\n","        data['profile_id'].max()+1 # changed from user to profile id, num_items,\n","        ,data['album_id'].max()+1 # changed from item to album_id\n","    )\n","            \n","\n","def convert_spmat_to_sptensor(X):\n","    coo = X.tocoo().astype(np.float32)\n","    row = torch.Tensor(coo.row).long()\n","    col = torch.Tensor(coo.col).long()\n","    index = torch.stack([row, col])\n","    data = torch.FloatTensor(coo.data)\n","    return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n","\n","def get_uninteracted_item(train_dict, unique_users, all_items):\n","    uninteracted_dict = {}\n","    num_uninter = []\n","    for user in unique_users:\n","        interacted_items = set(train_dict[user].keys())\n","        uninteracted_items = set(all_items) - interacted_items\n","        uninteracted_dict[user] = list(uninteracted_items)\n","        num_uninter.append(len(uninteracted_items))\n","    return uninteracted_dict, num_uninter"]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"NbyfZhGosoS_"}},{"cell_type":"code","source":["class LightGCNWithNG(nn.Module):\n","    def __init__(self,\n","                 num_users,\n","                 num_items,\n","                 g_laplace,\n","                 g_adj,\n","                 prior,\n","                 popularity,\n","                 uninter_mat,\n","                 num_uninter,\n","                 cfg,\n","                 device='cpu'):\n","        super(LightGCNWithNG, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.g_laplace = g_laplace\n","        self.g_adj = g_adj\n","        self.device = device\n","\n","        self.dim = cfg.dim\n","        self.hop = cfg.hop\n","        self.num_negatives = cfg.num_negatives\n","        self.alpha = cfg.alpha\n","        self.prior = prior                          # (|V|,)\n","        self.popularity = popularity\n","        self.uninter_mat = uninter_mat\n","        self.num_uninter = num_uninter\n","\n","        self.User_Emb = nn.Embedding(self.num_users, self.dim)\n","        nn.init.xavier_normal_(self.User_Emb.weight)\n","        self.Item_Emb = nn.Embedding(self.num_items, self.dim)\n","        nn.init.xavier_normal_(self.Item_Emb.weight)\n","\n","        # LightGCN Agg\n","        self.global_agg = []\n","        for i in range(self.hop):\n","            agg = LightGCNAgg(self.dim)\n","            self.add_module('Agg_LightGCN_{}'.format(i), agg)\n","            self.global_agg.append(agg)\n","\n","    def computer(self):\n","        users_emb = self.User_Emb.weight\n","        items_emb = self.Item_Emb.weight\n","        all_emb = torch.cat((users_emb, items_emb), dim=0)\n","        embs = [all_emb]\n","        for i in range(self.hop):\n","            aggregator = self.global_agg[i]\n","            x = aggregator(A=self.g_laplace, x=embs[i])\n","            embs.append(x)\n","        embs = torch.stack(embs, dim=1)\n","        light_out = torch.mean(embs, dim=1)\n","        users, items = torch.split(light_out, [self.num_users, self.num_items])\n","        return users, items\n","\n","    def sigmoid(self,x):\n","        return 1/(1+np.exp(-x))\n","\n","    def bns(self, users,items, ui_scores):\n","        batch_size = users.size(0)\n","        if self.device == 'cpu':\n","            users = users.detach().numpy()\n","            ui_scores = ui_scores.detach().numpy()\n","        else:\n","            users = users.cpu().detach().numpy()\n","            ui_scores = ui_scores.cpu().detach().numpy()\n","        negatives = []\n","        for bs in range(batch_size):\n","            u = users[bs]\n","            i = items[bs]\n","            rating_vector = ui_scores[bs]\n","            x_ui = rating_vector[i]\n","            negative_items = self.uninter_mat[u]\n","\n","            candidate_set = np.random.choice(negative_items, size=self.num_negatives, replace=False)\n","            candidate_scores = [rating_vector[l] for l in candidate_set]\n","\n","            # step 1 : computing info(l)\n","            info = np.array([1 - self.sigmoid(x_ui - x_ul) for x_ul in candidate_scores])  # O(1)\n","            # step 2 : computing prior probability\n","            p_fn = np.array([self.prior[l] for l in candidate_set])  # O(1)\n","            # step 3 : computing empirical distribution function (likelihood)\n","            F_n = np.array([np.sum(rating_vector <= x_ul) / (self.num_items+1) for x_ul in candidate_scores])  # O(|I|)\n","            # step 4: computing posterior probability\n","            unbias = (1 - F_n) * (1 - p_fn) / (1 - F_n - p_fn + 2 * F_n * p_fn)  # O(1)\n","            # step 5: computing conditional sampling risk\n","            conditional_risk = (1 - unbias) * info - self.alpha * unbias * info  # O(1)\n","            j = candidate_set[conditional_risk.argsort()[0]]\n","            negatives.append(j)\n","        negatives = torch.LongTensor(negatives)\n","        negatives = negatives.to(self.device)\n","        return negatives\n","\n","    def forward(self, epoch, users, items):\n","        all_users_emb, all_items_emb = self.computer()      # |U| * d, |V| * d\n","        users_emb = all_users_emb[users]    # bs * d\n","        items_emb = all_items_emb[items]    # bs * d\n","\n","        ui_scores = torch.mm(users_emb, all_items_emb.t())  # bs * |V|\n","        negatives = self.bns(users,items, ui_scores)  # bs\n","        neg_item_emb = all_items_emb[negatives]  # bs * d\n","\n","        pos_scores = torch.mul(users_emb, items_emb)\n","        pos_scores = pos_scores.sum(dim=1)      # (bs,)\n","        neg_scores = torch.mul(users_emb, neg_item_emb)\n","        neg_scores = neg_scores.sum(dim=1)      # (bs,)\n","\n","        bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))\n","\n","        return bpr_loss\n","\n","    def predict(self):\n","        all_users_emb, all_items_emb = self.computer()      # |U| * d, |V| * d\n","        rate_mat = torch.mm(all_users_emb, all_items_emb.t())\n","        return rate_mat\n","\n","class LightGCNAgg(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(LightGCNAgg, self).__init__()\n","        self.dim = hidden_size\n","\n","    def forward(self, A, x):\n","        '''\n","            A: n \\times n\n","            x: n \\times d\n","        '''\n","        return torch.sparse.mm(A, x)"],"metadata":{"id":"SVzoulSmsLKv","executionInfo":{"status":"ok","timestamp":1669960562110,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# main_lightGCN.py"],"metadata":{"id":"sKAbzd4Ns46h"}},{"cell_type":"code","source":["def train_test(model, train_data, test_data, train_slices, optimizer, epoch):\n","  print('start training: ', datetime.datetime.now())\n","  model.train()\n","  total_loss = []\n","  for index in train_slices:\n","      optimizer.zero_grad()\n","      users, items = train_data.get_slices(index)\n","      users = torch.LongTensor(users).to(device)\n","      items = torch.LongTensor(items).to(device)\n","\n","      bpr_loss = model(epoch, users, items)\n","\n","      bpr_loss.backward()\n","\n","      optimizer.step()\n","\n","      total_loss.append(bpr_loss.item())\n","\n","  print('Loss:\\t%.8f\\tlr:\\t%0.8f' % (np.mean(total_loss), optimizer.state_dict()['param_groups'][0]['lr']))\n","\n","  print('----------------')\n","  print('start predicting: ', datetime.datetime.now())\n","  \n","  pred_list = [] ####################\n","  model.eval()\n","  pre_dic, rec_dic, F1_dict, ndcg_dict = {}, {}, {}, {}\n","\n","  #################################################\n","  query_user_ids = total_data['profile_id'].unique()\n","  full_item_ids = np.array([c for c in range(num_items)])\n","  \n","  for user_id in query_user_ids:\n","    with torch.no_grad(): # 기울기 계산 안 할때\n","      user_ids = np.full(num_items, user_id)\n","      user_ids = torch.LongTensor(user_ids).to(device)\n","      item_ids = torch.LongTensor(full_item_ids).to(device)\n","\n","      eval_output = model.forward(user_ids, item_ids)\n","      pred_u_score = eval.output.reshape(-1)\n","\n","    pred_u_idx = np.argsort(pred_u_score)[::-1]\n","    pred_u = full_item_ids[pred_u_idx]\n","    pred_list.append(list(pred_u[:25])) # 25 top-k\n","\n","  pred = pd.DataFrame()\n","  pred['profile_id'] = query_user_ids\n","  pred['predicted_list'] = pred_list\n","  \n","  rets = evaluation(train_data, pred)\n","  #############################################\n","  \n","  rating_mat = model.predict()    # |U| * |V|\n","  if device == 'cpu':\n","      rating_mat = rating_mat.detach().numpy()\n","  else:\n","      rating_mat = rating_mat.cpu().detach().numpy()\n","  rating_mat = erase(rating_mat, train_data.train_dict)\n","\n","  for k in cfg.topk:\n","      matrices = topk_eval(rating_mat, test_data.test_label, k)\n","      precision, recall, F1, ndcg = matrices[0], matrices[1], matrices[2], matrices[3]\n","      pre_dic[k] = precision\n","      rec_dic[k] = recall\n","      F1_dict[k] = F1\n","      ndcg_dict[k] = ndcg\n","\n","  return pre_dic, rec_dic, F1_dict, ndcg_dict, pred\n","\n","\n","def erase(score, train_dict):\n","  for user in train_dict:\n","      for item in train_dict[user]:\n","          score[user, item] = -1000.0\n","  return score\n","\n","\n","def topk_eval(score, label, k):\n","  '''\n","  :param score: prediction\n","  :param k: number of top-k\n","  '''\n","  evaluation = [0, 0, 0, 0]\n","  counter = 0\n","  discountlist = [1 / math.log(i + 1, 2) for i in range(1, k + 1)]\n","\n","  for user_no in range(score.shape[0]):\n","      user_score = score[user_no].tolist()\n","      user_label = label[user_no].tolist()\n","      label_count = int(sum(user_label))\n","      topn_recommend_score = heapq.nlargest(k, user_score)  \n","      topn_recommend_index = [user_score.index(i) for i in\n","                              topn_recommend_score]  # map(user_score.index,topn_recommend_score)\n","      topn_recommend_label = [user_label[i] for i in topn_recommend_index]  \n","      idcg = discountlist[0:label_count]\n","\n","      if label_count == 0:\n","          counter += 1\n","          continue\n","      else:\n","          topk_label = topn_recommend_label[0:k]\n","          true_positive = sum(topk_label)\n","          evaluation[0] += true_positive / k  # precision\n","          evaluation[1] += true_positive / label_count  # recall\n","          evaluation[2] += 2 * true_positive / (k + label_count)  # f1\n","          evaluation[3] += np.dot(topk_label, discountlist[0:]) / sum(idcg)  # ndcg\n","  return [i / (score.shape[0] - counter) for i in evaluation]"],"metadata":{"id":"9JjS628V1vt7","executionInfo":{"status":"ok","timestamp":1669960562110,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def recallk(actual, predicted, k = 25):\n","    \"\"\" label과 prediction 사이의 recall 평가 함수 \n","    Args:\n","        actual : 실제로 본 상품 리스트\n","        pred : 예측한 상품 리스트\n","        k : 상위 몇개의 데이터를 볼지 (ex : k=5 상위 5개의 상품만 봄)\n","    Returns: \n","        recall_k : recall@k \n","    \"\"\" \n","    set_actual = set(actual)\n","    recall_k = len(set_actual & set(predicted[:k])) / min(k, len(set_actual))\n","    return recall_k\n","\n","def unique(sequence):\n","    # preserves order\n","    seen = set()\n","    return [x for x in sequence if not (x in seen or seen.add(x))]\n","\n","def ndcgk(actual, predicted, k = 25):\n","    set_actual = set(actual)\n","    idcg = sum([1.0 / np.log(i + 2) for i in range(min(k, len(set_actual)))])\n","    dcg = 0.0\n","    unique_predicted = unique(predicted[:k])\n","    for i, r in enumerate(unique_predicted):\n","        if r in set_actual:\n","            dcg += 1.0 / np.log(i + 2)\n","    ndcg_k = dcg / idcg\n","    return ndcg_k\n","\n","def evaluation(gt, pred):\n","    \"\"\" label과 prediction 사이의 recall, coverage, competition metric 평가 함수 \n","    Args:\n","        gt : 데이터 프레임 형태의 정답 데이터 \n","        pred : 데이터 프레임 형태의 예측 데이터 \n","    Returns: \n","        rets : recall, ndcg, coverage, competition metric 결과 \n","            ex) {'recall': 0.123024, 'ndcg': 056809, 'coverage': 0.017455, 'score': 0.106470}\n","    \"\"\"    \n","    gt = gt.groupby('profile_id')['album_id'].unique().to_frame().reset_index()\n","    gt.columns = ['profile_id', 'actual_list']\n","\n","    evaluated_data = pd.merge(pred, gt, how = 'left', on = 'profile_id')\n","\n","    evaluated_data['Recall@25'] = evaluated_data.apply(lambda x: recallk(x.actual_list, x.predicted_list), axis=1)\n","    evaluated_data['NDCG@25'] = evaluated_data.apply(lambda x: ndcgk(x.actual_list, x.predicted_list), axis=1)\n","\n","    recall = evaluated_data['Recall@25'].mean()\n","    ndcg = evaluated_data['NDCG@25'] .mean()\n","\n","    score = 0.75*recall + 0.25*ndcg\n","    rets = {\"recall\" :recall, \n","            \"ndcg\" :ndcg, \n","            \"score\" :score}\n","    return rets"],"metadata":{"id":"yfnYQWONOb4y","executionInfo":{"status":"ok","timestamp":1669960562111,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def init_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device('cuda' if USE_CUDA else 'cpu')\n","\n","t0 = time.time()\n","init_seed(42)\n","\n","# manually added part start\n","total_data = pd.read_csv('/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/updated_history_data.csv', encoding='utf-8') # load total file\n","total_data = total_data[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by = ['profile_id', 'log_time']).reset_index(drop = True) # drop duplicates\n","total_data = total_data[['profile_id', 'album_id']] # use only profile_id and album_id\n","total_data['rating'] = 1 # rating col with 1 added\n","train, test = train_test_split(total_data, test_size=0.2, random_state=42) # train test split 8:2\n","train = train.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","total_data.to_csv('/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/total.csv') # save total file\n","total_file = '/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/total.csv'\n","train.to_csv('/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/train.csv') # save train file\n","test.to_csv('/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/test.csv') # save test file\n","train_file = '/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/train.csv' # set train file path\n","test_file = '/content/drive/MyDrive/Next AI/2022 유플러스 AI Ground/data/test.csv' # set test file path\n","# manually added part end\n","\n","num_users, num_items = get_number_of_users_items(total_file)\n","train_data = Data(train_file, num_users, num_items, status='train')\n","test_data = Data(test_file, num_users, num_items, status='test')\n","\n","train_slices = train_data.generate_batch(cfg.batch_size)\n","\n","G_Lap_tensor = convert_spmat_to_sptensor(train_data.Lap_mat)\n","G_Adj_tensor = convert_spmat_to_sptensor(train_data.Adj_mat)\n","G_Lap_tensor = G_Lap_tensor.to(device)\n","G_Adj_tensor = G_Adj_tensor.to(device)\n","\n","uninter_mat, num_uninter = get_uninteracted_item(train_data.train_dict, train_data.train_user, list(total_data['album_id'].astype(int).unique()))\n","\n","model = LightGCNWithNG(num_users, num_items, G_Lap_tensor, G_Adj_tensor, train_data.prior,train_data.popularity,\n","                        uninter_mat, num_uninter, cfg, device)\n","model = model.to(device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.l2)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.lr_dc_epoch, gamma=cfg.lr_dc)\n","\n","best_result = {}\n","best_epoch = {}\n","for k in cfg.topk:\n","    best_result[k] = [0., 0., 0., 0.]\n","    best_epoch[k] = [0, 0, 0, 0]\n","# bad_counter = 0\n","for epoch in range(cfg.epochs):\n","    st = time.time()\n","    print('-------------------------------------------')\n","    print('epoch: ', epoch)\n","    pre_dic, rec_dic, F1_dict, ndcg_dict, pred = train_test(model, train_data, test_data, train_slices, optimizer, epoch)\n","    scheduler.step()\n","    for k in cfg.topk:\n","        if pre_dic[k] > best_result[k][0]:\n","            best_result[k][0] = pre_dic[k]\n","            best_epoch[k][0] = epoch\n","        if rec_dic[k] > best_result[k][1]:\n","            best_result[k][1] = rec_dic[k]\n","            best_epoch[k][1] = epoch\n","        if F1_dict[k] > best_result[k][2]:\n","            best_result[k][2] = F1_dict[k]\n","            best_epoch[k][2] = epoch\n","        if ndcg_dict[k] > best_result[k][3]:\n","            best_result[k][3] = ndcg_dict[k]\n","            best_epoch[k][3] = epoch\n","        print('Pre@%d:\\t%0.4f\\tRecall@%d:\\t%0.4f\\tF1@%d:\\t%0.4f\\tNDCG@%d:\\t%0.4f\\t[%0.2f s]' %\n","              (k, pre_dic[k], k, rec_dic[k], k, F1_dict[k], k, ndcg_dict[k], (time.time() - st)))\n","        \n","print('------------------best result-------------------')\n","for k in cfg.topk:\n","    print('Best Result: Pre@%d: %0.4f\\tRecall@%d: %0.4f\\tF1@%d: \\t%0.4f\\tNDCG@%d: \\t%0.4f [%0.2f s]' %\n","          (k, best_result[k][0], k, best_result[k][1], k, best_result[k][2], k, best_result[k][3], (time.time() - t0)))\n","    print('Best Epoch: Pre@%d: %d\\tRecall@%d: %d\\tF1@%d: %d\\tNDCG@%d: %d\\t [%0.2f s]' % (\n","        k, best_epoch[k][0], k, best_epoch[k][1], k, best_epoch[k][2], k, best_epoch[k][3], (time.time() - t0)))\n","print('------------------------------------------------')\n","print('Run time: %0.2f s' % (time.time() - t0))"],"metadata":{"id":"Kt0z5xX3s5Qy","executionInfo":{"status":"error","timestamp":1669966691997,"user_tz":-540,"elapsed":6129902,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"colab":{"base_uri":"https://localhost:8080/","height":610},"outputId":"ce9304b1-4ea2-4f29-bcf4-14a505fd904d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["building graph adjacency matrix\n","costing 62.08627653121948s, obtained norm_mat...\n","LightGCNWithNG(\n","  (User_Emb): Embedding(33033, 32)\n","  (Item_Emb): Embedding(25917, 32)\n","  (Agg_LightGCN_0): LightGCNAgg()\n",")\n","-------------------------------------------\n","epoch:  0\n","start training:  2022-12-02 06:37:23.097927\n","Loss:\t0.56860222\tlr:\t0.00100000\n","----------------\n","start predicting:  2022-12-02 07:38:09.472120\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9c4475cfab87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mpre_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-fc277739aaec>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, train_data, test_data, train_slices, optimizer, epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mitem_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_item_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0meval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mpred_u_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'items'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eyaZCSUaaBbL"},"execution_count":null,"outputs":[]}]}